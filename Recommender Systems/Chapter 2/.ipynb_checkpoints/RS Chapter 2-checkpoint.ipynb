{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8362b027",
   "metadata": {},
   "source": [
    "# Chapter 2 Dawei Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c190b5d4",
   "metadata": {},
   "source": [
    "For this chapter, you will explore neighborhood models for recommendation in more depth. You will also deal with some practical aspects of recommender systems functionality (such as going from algorithmic aspects of rating predictions to actually presenting users with top-n recommender items). You will also explore some additional features of the Scikit Surprise package to evaluate recommendation algorithms. Much of this assignment invovles making relatively small changes to the code in the provided KNN-Based Recommendation Module. However, you may also do your own implementations instead of using this code as the base so long as the functionality remain largely the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283884ff",
   "metadata": {},
   "source": [
    "## This notebook contains the following two parts\n",
    "### Part 1. Neighborhood models for Joke Recommendation \n",
    "### Part 2. More Exploration of the Scikit Surpise Package for Recommendation\n",
    "#### Note: this note book requires KNNRecommender.py under the same folder, which provides similarity and knn search related functions. KNN Book Recommender Example.ipynb providing detailed examples of using the functions in the KNNRecommender module. The dataset used here is from https://eigentaste.berkeley.edu/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9677548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version list\n",
      "\n",
      "python 3.9.7\n",
      "numpy 1.22.2\n",
      "pandas 1.3.5\n",
      "scipy 1.7.3\n",
      "seaborn 0.11.2\n",
      "matplotlib 3.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from platform import python_version\n",
    "from scipy.spatial.distance import cosine, correlation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "pd.set_option('max_columns',200)\n",
    "pd.set_option('display.precision',2)\n",
    "\n",
    "print('Version list\\n')\n",
    "print('python',python_version())\n",
    "print(np.__name__, np.__version__)\n",
    "print(pd.__name__, pd.__version__)\n",
    "print(sp.__name__, sp.__version__)\n",
    "print(sns.__name__, sns.__version__)\n",
    "print(matplotlib.__name__, matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe72d17",
   "metadata": {},
   "source": [
    "## Part 1. Neighborhood models for Joke Recommendation \n",
    "### 1a \n",
    "Read in the joke ratings data as well as joke text data into appropriate data structures. You can start with the provided shell Jupyter Notebook. Begin by doing some simple testing the functions provided in the recommendation module to make sure everything works:  <br>&emsp;· Run \"knn_search\" with at two existing (target) users in the ratings matrix and dispalying the list of top 10 neighbors and their similarities to the target user in decreasing order of similarities. Perform this task twice for each user, once with Pearson and once with Cosine as similarity measures (use the similarity functions provided in the module).<br>&emsp;· Similarly, run the \"knn_predict\" function for at least 2 user-item pairs with k=20 as the number of neighbors and display the predicted ratings in each case.<br>&emsp;· Run the \"test\" function to report the MAE (Mean Absolute Error) for the top 50 users on the ratings matrix as test users, using a 0.2 test_ratio (i.e., for each test user, a randomly selected 20% of the known ratings will be used for evaluation), and k = 10 (10 neighbors). Note: depending on your machine's capabilities, this may take a few minutes.<br>&emsp;· Use the \"recommend\" function to generate and display the top 3 joke recommendations for at least tw0 users using k=20 as the number of neighbors and Pearson similarity measure. You should display the predicted rating as well as the text of the recommended jokes in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914a8318",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Chapter2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8732189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a function to transfer text document into appropriate data structure\n",
    "def load_jokes(file):\n",
    "    jokes = np.genfromtxt(file, delimiter=',', dtype=str)\n",
    "    jokes = np.array(jokes[:,1])\n",
    "    return jokes\n",
    "joke = load_jokes(\"jokes.csv\")\n",
    "rate = np.loadtxt('joke-ratings.csv', delimiter =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d98fef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17f93dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A man visits the doctor. The doctor says \"I have bad news for you.You have cancer and Alzheimer\\'s disease\". The man replies \"Well thank God I don\\'t have cancer!\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd62db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0764969d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.18, 19.79,  1.34, ...,  0.  ,  0.  ,  0.  ],\n",
       "       [15.08, 10.71, 17.36, ..., 11.34,  6.68, 12.07],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n",
       "       ...,\n",
       "       [16.58, 16.63, 15.85, ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 3.67,  4.45,  3.67, ...,  3.77,  3.77,  3.28],\n",
       "       [ 9.88, 11.73,  9.16, ...,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7673acf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.18</td>\n",
       "      <td>19.79</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.15</td>\n",
       "      <td>15.17</td>\n",
       "      <td>2.02</td>\n",
       "      <td>6.24</td>\n",
       "      <td>2.50</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.82</td>\n",
       "      <td>19.45</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.57</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.63</td>\n",
       "      <td>12.50</td>\n",
       "      <td>6.63</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.50</td>\n",
       "      <td>12.12</td>\n",
       "      <td>18.82</td>\n",
       "      <td>13.86</td>\n",
       "      <td>20.13</td>\n",
       "      <td>3.57</td>\n",
       "      <td>13.14</td>\n",
       "      <td>6.92</td>\n",
       "      <td>1.92</td>\n",
       "      <td>18.82</td>\n",
       "      <td>16.05</td>\n",
       "      <td>15.95</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.86</td>\n",
       "      <td>4.74</td>\n",
       "      <td>14.79</td>\n",
       "      <td>10.90</td>\n",
       "      <td>14.93</td>\n",
       "      <td>15.13</td>\n",
       "      <td>2.31</td>\n",
       "      <td>3.86</td>\n",
       "      <td>14.20</td>\n",
       "      <td>19.30</td>\n",
       "      <td>6.44</td>\n",
       "      <td>11.92</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.58</td>\n",
       "      <td>13.82</td>\n",
       "      <td>2.36</td>\n",
       "      <td>19.59</td>\n",
       "      <td>14.59</td>\n",
       "      <td>4.16</td>\n",
       "      <td>1.97</td>\n",
       "      <td>13.82</td>\n",
       "      <td>9.64</td>\n",
       "      <td>1.92</td>\n",
       "      <td>19.30</td>\n",
       "      <td>16.68</td>\n",
       "      <td>6.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.08</td>\n",
       "      <td>10.71</td>\n",
       "      <td>17.36</td>\n",
       "      <td>15.37</td>\n",
       "      <td>8.62</td>\n",
       "      <td>1.34</td>\n",
       "      <td>10.27</td>\n",
       "      <td>5.66</td>\n",
       "      <td>19.88</td>\n",
       "      <td>20.22</td>\n",
       "      <td>17.75</td>\n",
       "      <td>19.64</td>\n",
       "      <td>15.42</td>\n",
       "      <td>18.43</td>\n",
       "      <td>15.56</td>\n",
       "      <td>10.03</td>\n",
       "      <td>15.66</td>\n",
       "      <td>10.32</td>\n",
       "      <td>14.30</td>\n",
       "      <td>9.79</td>\n",
       "      <td>11.87</td>\n",
       "      <td>19.64</td>\n",
       "      <td>19.35</td>\n",
       "      <td>20.17</td>\n",
       "      <td>11.05</td>\n",
       "      <td>18.57</td>\n",
       "      <td>15.71</td>\n",
       "      <td>11.87</td>\n",
       "      <td>10.61</td>\n",
       "      <td>17.99</td>\n",
       "      <td>17.50</td>\n",
       "      <td>10.08</td>\n",
       "      <td>18.14</td>\n",
       "      <td>20.03</td>\n",
       "      <td>9.20</td>\n",
       "      <td>11.73</td>\n",
       "      <td>18.09</td>\n",
       "      <td>14.40</td>\n",
       "      <td>10.13</td>\n",
       "      <td>18.91</td>\n",
       "      <td>18.82</td>\n",
       "      <td>7.17</td>\n",
       "      <td>19.64</td>\n",
       "      <td>19.98</td>\n",
       "      <td>6.68</td>\n",
       "      <td>7.80</td>\n",
       "      <td>6.10</td>\n",
       "      <td>10.08</td>\n",
       "      <td>9.54</td>\n",
       "      <td>14.64</td>\n",
       "      <td>14.16</td>\n",
       "      <td>20.03</td>\n",
       "      <td>11.97</td>\n",
       "      <td>9.69</td>\n",
       "      <td>4.50</td>\n",
       "      <td>7.80</td>\n",
       "      <td>19.64</td>\n",
       "      <td>8.86</td>\n",
       "      <td>11.10</td>\n",
       "      <td>20.03</td>\n",
       "      <td>4.30</td>\n",
       "      <td>7.65</td>\n",
       "      <td>1.97</td>\n",
       "      <td>15.47</td>\n",
       "      <td>15.08</td>\n",
       "      <td>7.17</td>\n",
       "      <td>19.74</td>\n",
       "      <td>12.12</td>\n",
       "      <td>11.78</td>\n",
       "      <td>18.52</td>\n",
       "      <td>6.00</td>\n",
       "      <td>13.77</td>\n",
       "      <td>19.30</td>\n",
       "      <td>18.77</td>\n",
       "      <td>18.33</td>\n",
       "      <td>17.21</td>\n",
       "      <td>18.72</td>\n",
       "      <td>19.98</td>\n",
       "      <td>19.64</td>\n",
       "      <td>19.20</td>\n",
       "      <td>14.93</td>\n",
       "      <td>15.85</td>\n",
       "      <td>15.85</td>\n",
       "      <td>17.07</td>\n",
       "      <td>19.98</td>\n",
       "      <td>15.51</td>\n",
       "      <td>10.95</td>\n",
       "      <td>14.69</td>\n",
       "      <td>15.56</td>\n",
       "      <td>11.58</td>\n",
       "      <td>13.82</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.71</td>\n",
       "      <td>18.86</td>\n",
       "      <td>10.81</td>\n",
       "      <td>8.86</td>\n",
       "      <td>14.06</td>\n",
       "      <td>11.34</td>\n",
       "      <td>6.68</td>\n",
       "      <td>12.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.33</td>\n",
       "      <td>18.57</td>\n",
       "      <td>20.37</td>\n",
       "      <td>17.17</td>\n",
       "      <td>4.64</td>\n",
       "      <td>4.11</td>\n",
       "      <td>3.14</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.03</td>\n",
       "      <td>18.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.48</td>\n",
       "      <td>18.28</td>\n",
       "      <td>18.28</td>\n",
       "      <td>19.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.17</td>\n",
       "      <td>18.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.98</td>\n",
       "      <td>18.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.17</td>\n",
       "      <td>20.08</td>\n",
       "      <td>18.33</td>\n",
       "      <td>18.52</td>\n",
       "      <td>20.27</td>\n",
       "      <td>20.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.17</td>\n",
       "      <td>18.33</td>\n",
       "      <td>20.08</td>\n",
       "      <td>18.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.28</td>\n",
       "      <td>18.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.27</td>\n",
       "      <td>17.46</td>\n",
       "      <td>18.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.04</td>\n",
       "      <td>18.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.28</td>\n",
       "      <td>19.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>19.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.80</td>\n",
       "      <td>19.16</td>\n",
       "      <td>8.18</td>\n",
       "      <td>17.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.84</td>\n",
       "      <td>18.33</td>\n",
       "      <td>17.60</td>\n",
       "      <td>17.31</td>\n",
       "      <td>19.11</td>\n",
       "      <td>3.77</td>\n",
       "      <td>4.35</td>\n",
       "      <td>12.17</td>\n",
       "      <td>4.40</td>\n",
       "      <td>7.36</td>\n",
       "      <td>8.91</td>\n",
       "      <td>16.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.91</td>\n",
       "      <td>14.93</td>\n",
       "      <td>17.75</td>\n",
       "      <td>17.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.65</td>\n",
       "      <td>4.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.57</td>\n",
       "      <td>17.21</td>\n",
       "      <td>17.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.70</td>\n",
       "      <td>18.18</td>\n",
       "      <td>13.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.84</td>\n",
       "      <td>17.84</td>\n",
       "      <td>7.02</td>\n",
       "      <td>17.99</td>\n",
       "      <td>14.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.94</td>\n",
       "      <td>12.55</td>\n",
       "      <td>18.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>7.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.82</td>\n",
       "      <td>11.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.28</td>\n",
       "      <td>8.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.50</td>\n",
       "      <td>15.61</td>\n",
       "      <td>6.83</td>\n",
       "      <td>5.61</td>\n",
       "      <td>12.36</td>\n",
       "      <td>12.60</td>\n",
       "      <td>18.04</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.56</td>\n",
       "      <td>16.73</td>\n",
       "      <td>19.25</td>\n",
       "      <td>17.84</td>\n",
       "      <td>7.07</td>\n",
       "      <td>18.23</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1.34</td>\n",
       "      <td>13.72</td>\n",
       "      <td>9.64</td>\n",
       "      <td>13.57</td>\n",
       "      <td>15.51</td>\n",
       "      <td>19.20</td>\n",
       "      <td>17.12</td>\n",
       "      <td>19.30</td>\n",
       "      <td>9.74</td>\n",
       "      <td>18.77</td>\n",
       "      <td>12.89</td>\n",
       "      <td>9.83</td>\n",
       "      <td>16.68</td>\n",
       "      <td>19.45</td>\n",
       "      <td>15.61</td>\n",
       "      <td>19.06</td>\n",
       "      <td>1.53</td>\n",
       "      <td>18.28</td>\n",
       "      <td>16.68</td>\n",
       "      <td>13.48</td>\n",
       "      <td>14.20</td>\n",
       "      <td>9.74</td>\n",
       "      <td>17.80</td>\n",
       "      <td>15.51</td>\n",
       "      <td>13.48</td>\n",
       "      <td>11.34</td>\n",
       "      <td>17.84</td>\n",
       "      <td>11.19</td>\n",
       "      <td>2.26</td>\n",
       "      <td>16.24</td>\n",
       "      <td>17.31</td>\n",
       "      <td>19.06</td>\n",
       "      <td>12.26</td>\n",
       "      <td>15.51</td>\n",
       "      <td>10.95</td>\n",
       "      <td>15.42</td>\n",
       "      <td>14.06</td>\n",
       "      <td>19.93</td>\n",
       "      <td>18.82</td>\n",
       "      <td>12.75</td>\n",
       "      <td>19.11</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1.83</td>\n",
       "      <td>15.95</td>\n",
       "      <td>14.35</td>\n",
       "      <td>18.38</td>\n",
       "      <td>17.17</td>\n",
       "      <td>15.71</td>\n",
       "      <td>8.72</td>\n",
       "      <td>18.38</td>\n",
       "      <td>15.56</td>\n",
       "      <td>18.14</td>\n",
       "      <td>15.22</td>\n",
       "      <td>14.01</td>\n",
       "      <td>14.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.24</td>\n",
       "      <td>16.92</td>\n",
       "      <td>11.87</td>\n",
       "      <td>18.28</td>\n",
       "      <td>14.93</td>\n",
       "      <td>10.37</td>\n",
       "      <td>17.31</td>\n",
       "      <td>15.71</td>\n",
       "      <td>13.82</td>\n",
       "      <td>13.96</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.58</td>\n",
       "      <td>15.27</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.73</td>\n",
       "      <td>12.55</td>\n",
       "      <td>14.11</td>\n",
       "      <td>17.55</td>\n",
       "      <td>12.80</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>12.94</td>\n",
       "      <td>5.47</td>\n",
       "      <td>16.19</td>\n",
       "      <td>5.51</td>\n",
       "      <td>6.92</td>\n",
       "      <td>8.48</td>\n",
       "      <td>14.20</td>\n",
       "      <td>14.83</td>\n",
       "      <td>4.98</td>\n",
       "      <td>13.96</td>\n",
       "      <td>7.26</td>\n",
       "      <td>9.74</td>\n",
       "      <td>11.73</td>\n",
       "      <td>7.65</td>\n",
       "      <td>8.18</td>\n",
       "      <td>7.84</td>\n",
       "      <td>12.12</td>\n",
       "      <td>12.31</td>\n",
       "      <td>13.91</td>\n",
       "      <td>12.17</td>\n",
       "      <td>9.20</td>\n",
       "      <td>6.44</td>\n",
       "      <td>11.39</td>\n",
       "      <td>5.85</td>\n",
       "      <td>7.17</td>\n",
       "      <td>11.29</td>\n",
       "      <td>13.96</td>\n",
       "      <td>10.03</td>\n",
       "      <td>15.17</td>\n",
       "      <td>5.13</td>\n",
       "      <td>7.80</td>\n",
       "      <td>17.89</td>\n",
       "      <td>7.02</td>\n",
       "      <td>5.76</td>\n",
       "      <td>14.83</td>\n",
       "      <td>15.37</td>\n",
       "      <td>7.70</td>\n",
       "      <td>13.91</td>\n",
       "      <td>7.80</td>\n",
       "      <td>12.41</td>\n",
       "      <td>9.01</td>\n",
       "      <td>3.67</td>\n",
       "      <td>5.13</td>\n",
       "      <td>8.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>7.21</td>\n",
       "      <td>9.74</td>\n",
       "      <td>11.00</td>\n",
       "      <td>13.57</td>\n",
       "      <td>13.28</td>\n",
       "      <td>5.85</td>\n",
       "      <td>8.28</td>\n",
       "      <td>6.10</td>\n",
       "      <td>12.65</td>\n",
       "      <td>4.16</td>\n",
       "      <td>9.30</td>\n",
       "      <td>7.50</td>\n",
       "      <td>3.18</td>\n",
       "      <td>6.97</td>\n",
       "      <td>12.17</td>\n",
       "      <td>12.36</td>\n",
       "      <td>11.10</td>\n",
       "      <td>5.27</td>\n",
       "      <td>8.57</td>\n",
       "      <td>5.66</td>\n",
       "      <td>11.97</td>\n",
       "      <td>4.25</td>\n",
       "      <td>13.38</td>\n",
       "      <td>12.55</td>\n",
       "      <td>6.00</td>\n",
       "      <td>14.64</td>\n",
       "      <td>13.09</td>\n",
       "      <td>8.48</td>\n",
       "      <td>7.02</td>\n",
       "      <td>4.30</td>\n",
       "      <td>5.95</td>\n",
       "      <td>7.36</td>\n",
       "      <td>6.63</td>\n",
       "      <td>7.55</td>\n",
       "      <td>2.46</td>\n",
       "      <td>3.48</td>\n",
       "      <td>7.12</td>\n",
       "      <td>10.95</td>\n",
       "      <td>10.03</td>\n",
       "      <td>7.89</td>\n",
       "      <td>7.12</td>\n",
       "      <td>11.15</td>\n",
       "      <td>8.77</td>\n",
       "      <td>13.86</td>\n",
       "      <td>10.71</td>\n",
       "      <td>6.58</td>\n",
       "      <td>9.93</td>\n",
       "      <td>15.37</td>\n",
       "      <td>7.89</td>\n",
       "      <td>13.72</td>\n",
       "      <td>6.87</td>\n",
       "      <td>13.23</td>\n",
       "      <td>5.47</td>\n",
       "      <td>14.54</td>\n",
       "      <td>13.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>15.27</td>\n",
       "      <td>11.39</td>\n",
       "      <td>16.39</td>\n",
       "      <td>5.37</td>\n",
       "      <td>7.41</td>\n",
       "      <td>16.58</td>\n",
       "      <td>12.17</td>\n",
       "      <td>2.84</td>\n",
       "      <td>5.13</td>\n",
       "      <td>4.30</td>\n",
       "      <td>16.39</td>\n",
       "      <td>18.23</td>\n",
       "      <td>9.01</td>\n",
       "      <td>15.32</td>\n",
       "      <td>14.83</td>\n",
       "      <td>1.24</td>\n",
       "      <td>5.76</td>\n",
       "      <td>7.89</td>\n",
       "      <td>9.25</td>\n",
       "      <td>9.83</td>\n",
       "      <td>15.32</td>\n",
       "      <td>15.95</td>\n",
       "      <td>11.19</td>\n",
       "      <td>3.09</td>\n",
       "      <td>16.05</td>\n",
       "      <td>16.05</td>\n",
       "      <td>17.84</td>\n",
       "      <td>12.36</td>\n",
       "      <td>14.50</td>\n",
       "      <td>9.79</td>\n",
       "      <td>15.81</td>\n",
       "      <td>16.00</td>\n",
       "      <td>10.42</td>\n",
       "      <td>10.95</td>\n",
       "      <td>16.39</td>\n",
       "      <td>19.16</td>\n",
       "      <td>4.83</td>\n",
       "      <td>18.14</td>\n",
       "      <td>15.76</td>\n",
       "      <td>10.13</td>\n",
       "      <td>6.24</td>\n",
       "      <td>10.08</td>\n",
       "      <td>11.15</td>\n",
       "      <td>7.12</td>\n",
       "      <td>15.85</td>\n",
       "      <td>5.27</td>\n",
       "      <td>14.06</td>\n",
       "      <td>13.14</td>\n",
       "      <td>19.69</td>\n",
       "      <td>17.89</td>\n",
       "      <td>7.46</td>\n",
       "      <td>13.52</td>\n",
       "      <td>17.75</td>\n",
       "      <td>12.17</td>\n",
       "      <td>5.47</td>\n",
       "      <td>7.80</td>\n",
       "      <td>7.55</td>\n",
       "      <td>14.40</td>\n",
       "      <td>13.91</td>\n",
       "      <td>2.55</td>\n",
       "      <td>16.44</td>\n",
       "      <td>14.83</td>\n",
       "      <td>8.18</td>\n",
       "      <td>16.00</td>\n",
       "      <td>18.23</td>\n",
       "      <td>2.41</td>\n",
       "      <td>7.31</td>\n",
       "      <td>16.68</td>\n",
       "      <td>16.44</td>\n",
       "      <td>11.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>16.58</td>\n",
       "      <td>16.63</td>\n",
       "      <td>15.85</td>\n",
       "      <td>7.89</td>\n",
       "      <td>14.40</td>\n",
       "      <td>9.74</td>\n",
       "      <td>14.54</td>\n",
       "      <td>13.14</td>\n",
       "      <td>6.34</td>\n",
       "      <td>11.78</td>\n",
       "      <td>14.88</td>\n",
       "      <td>13.72</td>\n",
       "      <td>13.48</td>\n",
       "      <td>10.76</td>\n",
       "      <td>13.82</td>\n",
       "      <td>7.99</td>\n",
       "      <td>10.85</td>\n",
       "      <td>13.14</td>\n",
       "      <td>10.95</td>\n",
       "      <td>11.05</td>\n",
       "      <td>6.49</td>\n",
       "      <td>10.95</td>\n",
       "      <td>9.83</td>\n",
       "      <td>4.01</td>\n",
       "      <td>12.21</td>\n",
       "      <td>6.97</td>\n",
       "      <td>15.32</td>\n",
       "      <td>8.72</td>\n",
       "      <td>14.83</td>\n",
       "      <td>10.03</td>\n",
       "      <td>14.74</td>\n",
       "      <td>16.58</td>\n",
       "      <td>13.43</td>\n",
       "      <td>10.08</td>\n",
       "      <td>13.67</td>\n",
       "      <td>14.06</td>\n",
       "      <td>3.48</td>\n",
       "      <td>10.32</td>\n",
       "      <td>11.24</td>\n",
       "      <td>8.48</td>\n",
       "      <td>6.34</td>\n",
       "      <td>11.87</td>\n",
       "      <td>6.44</td>\n",
       "      <td>7.50</td>\n",
       "      <td>9.16</td>\n",
       "      <td>15.03</td>\n",
       "      <td>8.09</td>\n",
       "      <td>13.86</td>\n",
       "      <td>9.74</td>\n",
       "      <td>14.35</td>\n",
       "      <td>10.56</td>\n",
       "      <td>10.13</td>\n",
       "      <td>15.22</td>\n",
       "      <td>11.87</td>\n",
       "      <td>13.28</td>\n",
       "      <td>15.51</td>\n",
       "      <td>4.01</td>\n",
       "      <td>7.75</td>\n",
       "      <td>10.08</td>\n",
       "      <td>12.31</td>\n",
       "      <td>13.43</td>\n",
       "      <td>14.45</td>\n",
       "      <td>13.14</td>\n",
       "      <td>8.96</td>\n",
       "      <td>10.32</td>\n",
       "      <td>12.21</td>\n",
       "      <td>4.45</td>\n",
       "      <td>14.20</td>\n",
       "      <td>12.55</td>\n",
       "      <td>10.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>3.67</td>\n",
       "      <td>4.45</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>9.40</td>\n",
       "      <td>7.65</td>\n",
       "      <td>3.86</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.67</td>\n",
       "      <td>4.93</td>\n",
       "      <td>4.45</td>\n",
       "      <td>3.38</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.38</td>\n",
       "      <td>9.93</td>\n",
       "      <td>3.48</td>\n",
       "      <td>4.40</td>\n",
       "      <td>4.40</td>\n",
       "      <td>9.98</td>\n",
       "      <td>3.86</td>\n",
       "      <td>9.79</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.38</td>\n",
       "      <td>6.78</td>\n",
       "      <td>11.49</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>7.36</td>\n",
       "      <td>7.84</td>\n",
       "      <td>3.67</td>\n",
       "      <td>7.65</td>\n",
       "      <td>8.43</td>\n",
       "      <td>8.52</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.38</td>\n",
       "      <td>7.12</td>\n",
       "      <td>10.17</td>\n",
       "      <td>10.03</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.38</td>\n",
       "      <td>7.02</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.33</td>\n",
       "      <td>7.70</td>\n",
       "      <td>6.63</td>\n",
       "      <td>4.45</td>\n",
       "      <td>3.77</td>\n",
       "      <td>6.83</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>7.21</td>\n",
       "      <td>8.33</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.67</td>\n",
       "      <td>7.17</td>\n",
       "      <td>6.83</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.38</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.30</td>\n",
       "      <td>3.86</td>\n",
       "      <td>6.63</td>\n",
       "      <td>4.11</td>\n",
       "      <td>4.11</td>\n",
       "      <td>7.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4.16</td>\n",
       "      <td>9.20</td>\n",
       "      <td>4.16</td>\n",
       "      <td>4.16</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.82</td>\n",
       "      <td>3.82</td>\n",
       "      <td>6.87</td>\n",
       "      <td>6.87</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>9.88</td>\n",
       "      <td>11.73</td>\n",
       "      <td>9.16</td>\n",
       "      <td>9.50</td>\n",
       "      <td>13.52</td>\n",
       "      <td>12.07</td>\n",
       "      <td>14.50</td>\n",
       "      <td>11.97</td>\n",
       "      <td>13.28</td>\n",
       "      <td>11.68</td>\n",
       "      <td>14.50</td>\n",
       "      <td>9.98</td>\n",
       "      <td>11.19</td>\n",
       "      <td>11.83</td>\n",
       "      <td>13.28</td>\n",
       "      <td>8.14</td>\n",
       "      <td>9.25</td>\n",
       "      <td>9.59</td>\n",
       "      <td>12.99</td>\n",
       "      <td>12.65</td>\n",
       "      <td>14.20</td>\n",
       "      <td>11.83</td>\n",
       "      <td>15.13</td>\n",
       "      <td>9.69</td>\n",
       "      <td>11.58</td>\n",
       "      <td>12.26</td>\n",
       "      <td>13.86</td>\n",
       "      <td>15.95</td>\n",
       "      <td>13.72</td>\n",
       "      <td>13.67</td>\n",
       "      <td>18.14</td>\n",
       "      <td>16.83</td>\n",
       "      <td>9.93</td>\n",
       "      <td>12.84</td>\n",
       "      <td>15.61</td>\n",
       "      <td>13.82</td>\n",
       "      <td>12.07</td>\n",
       "      <td>12.26</td>\n",
       "      <td>14.25</td>\n",
       "      <td>16.53</td>\n",
       "      <td>9.30</td>\n",
       "      <td>13.48</td>\n",
       "      <td>12.41</td>\n",
       "      <td>9.79</td>\n",
       "      <td>13.43</td>\n",
       "      <td>13.18</td>\n",
       "      <td>18.04</td>\n",
       "      <td>12.80</td>\n",
       "      <td>14.74</td>\n",
       "      <td>14.93</td>\n",
       "      <td>11.44</td>\n",
       "      <td>15.66</td>\n",
       "      <td>12.12</td>\n",
       "      <td>9.30</td>\n",
       "      <td>17.55</td>\n",
       "      <td>12.70</td>\n",
       "      <td>12.02</td>\n",
       "      <td>6.19</td>\n",
       "      <td>12.07</td>\n",
       "      <td>16.29</td>\n",
       "      <td>15.90</td>\n",
       "      <td>13.09</td>\n",
       "      <td>19.01</td>\n",
       "      <td>11.68</td>\n",
       "      <td>16.34</td>\n",
       "      <td>17.41</td>\n",
       "      <td>13.09</td>\n",
       "      <td>17.55</td>\n",
       "      <td>15.32</td>\n",
       "      <td>11.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1      2      3      4      5      6      7      8      9   \\\n",
       "0     3.18  19.79   1.34   2.84   3.48   2.50   1.15  15.17   2.02   6.24   \n",
       "1    15.08  10.71  17.36  15.37   8.62   1.34  10.27   5.66  19.88  20.22   \n",
       "2     0.00   0.00   0.00   0.00  20.03  20.27  20.03  20.27   0.00   0.00   \n",
       "3     0.00  19.35   0.00   0.00  12.80  19.16   8.18  17.21   0.00  12.84   \n",
       "4    19.50  15.61   6.83   5.61  12.36  12.60  18.04  15.61  10.56  16.73   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "995  12.94   5.47  16.19   5.51   6.92   8.48  14.20  14.83   4.98  13.96   \n",
       "996  15.27  11.39  16.39   5.37   7.41  16.58  12.17   2.84   5.13   4.30   \n",
       "997  16.58  16.63  15.85   7.89  14.40   9.74  14.54  13.14   6.34  11.78   \n",
       "998   3.67   4.45   3.67   3.67   9.40   7.65   3.86   4.40   3.67   4.93   \n",
       "999   9.88  11.73   9.16   9.50  13.52  12.07  14.50  11.97  13.28  11.68   \n",
       "\n",
       "        10     11     12     13     14     15     16     17     18     19  \\\n",
       "0     2.50   4.25   3.82  19.45   3.82   3.48   3.57   1.19   1.15   1.15   \n",
       "1    17.75  19.64  15.42  18.43  15.56  10.03  15.66  10.32  14.30   9.79   \n",
       "2    18.33  18.57  20.37  17.17   4.64   4.11   3.14  20.03  20.03  20.03   \n",
       "3    18.33  17.60  17.31  19.11   3.77   4.35  12.17   4.40   7.36   8.91   \n",
       "4    19.25  17.84   7.07  18.23   8.67   1.34  13.72   9.64  13.57  15.51   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "995   7.26   9.74  11.73   7.65   8.18   7.84  12.12  12.31  13.91  12.17   \n",
       "996  16.39  18.23   9.01  15.32  14.83   1.24   5.76   7.89   9.25   9.83   \n",
       "997  14.88  13.72  13.48  10.76  13.82   7.99  10.85  13.14  10.95  11.05   \n",
       "998   4.45   3.38   4.40   3.38   9.93   3.48   4.40   4.40   9.98   3.86   \n",
       "999  14.50   9.98  11.19  11.83  13.28   8.14   9.25   9.59  12.99  12.65   \n",
       "\n",
       "        20     21     22     23     24     25     26     27     28     29  \\\n",
       "0     1.63  12.50   6.63   1.19   2.50  12.12  18.82  13.86  20.13   3.57   \n",
       "1    11.87  19.64  19.35  20.17  11.05  18.57  15.71  11.87  10.61  17.99   \n",
       "2    18.28   0.00  19.25   0.00   0.00  18.48  18.28  18.28  19.93   0.00   \n",
       "3    16.34   0.00   0.00   0.00   0.00  13.91  14.93  17.75  17.60   0.00   \n",
       "4    19.20  17.12  19.30   9.74  18.77  12.89   9.83  16.68  19.45  15.61   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "995   9.20   6.44  11.39   5.85   7.17  11.29  13.96  10.03  15.17   5.13   \n",
       "996  15.32  15.95  11.19   3.09  16.05  16.05  17.84  12.36  14.50   9.79   \n",
       "997   6.49  10.95   9.83   4.01  12.21   6.97  15.32   8.72  14.83  10.03   \n",
       "998   9.79   3.38   3.38   3.67   3.38   6.78  11.49   3.33   3.62   3.62   \n",
       "999  14.20  11.83  15.13   9.69  11.58  12.26  13.86  15.95  13.72  13.67   \n",
       "\n",
       "        30     31     32     33     34     35     36     37     38     39  \\\n",
       "0    13.14   6.92   1.92  18.82  16.05  15.95   1.83   2.60   2.60   2.60   \n",
       "1    17.50  10.08  18.14  20.03   9.20  11.73  18.09  14.40  10.13  18.91   \n",
       "2    17.17  18.28   0.00   0.00  19.98  18.33   0.00  17.17  20.08  18.33   \n",
       "3    17.65   4.88   0.00  18.57  17.21  17.65   0.00   2.70  18.18  13.82   \n",
       "4    19.06   1.53  18.28  16.68  13.48  14.20   9.74  17.80  15.51  13.48   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "995   7.80  17.89   7.02   5.76  14.83  15.37   7.70  13.91   7.80  12.41   \n",
       "996  15.81  16.00  10.42  10.95  16.39  19.16   4.83  18.14  15.76  10.13   \n",
       "997  14.74  16.58  13.43  10.08  13.67  14.06   3.48  10.32  11.24   8.48   \n",
       "998   7.36   7.84   3.67   7.65   8.43   8.52   3.62   3.38   3.38   3.38   \n",
       "999  18.14  16.83   9.93  12.84  15.61  13.82  12.07  12.26  14.25  16.53   \n",
       "\n",
       "        40     41     42     43     44     45     46     47     48     49  \\\n",
       "0     2.89   1.87   1.97   1.92   3.86   4.74  14.79  10.90  14.93  15.13   \n",
       "1    18.82   7.17  19.64  19.98   6.68   7.80   6.10  10.08   9.54  14.64   \n",
       "2    18.52  20.27  20.27   0.00  20.27   0.00  17.17  18.33  20.08  18.28   \n",
       "3     0.00  12.55   0.00   0.00   0.00  17.84  17.84   7.02  17.99  14.45   \n",
       "4    11.34  17.84  11.19   2.26  16.24  17.31  19.06  12.26  15.51  10.95   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "995   9.01   3.67   5.13   8.18   7.80   7.21   9.74  11.00  13.57  13.28   \n",
       "996   6.24  10.08  11.15   7.12  15.85   5.27  14.06  13.14  19.69  17.89   \n",
       "997   6.34  11.87   6.44   7.50   9.16  15.03   8.09  13.86   9.74  14.35   \n",
       "998   7.12  10.17  10.03   3.86   3.38   7.02   3.62   3.38   3.33   7.70   \n",
       "999   9.30  13.48  12.41   9.79  13.43  13.18  18.04  12.80  14.74  14.93   \n",
       "\n",
       "        50     51     52     53     54     55     56     57     58     59  \\\n",
       "0     2.31   3.86  14.20  19.30   6.44  11.92   1.87   1.58  13.82   2.36   \n",
       "1    14.16  20.03  11.97   9.69   4.50   7.80  19.64   8.86  11.10  20.03   \n",
       "2     0.00   0.00  18.28  18.33   0.00  18.23   0.00   0.00   0.00  20.27   \n",
       "3     0.00  17.94  12.55  18.67   0.00  17.55   0.00   0.00   0.00   0.00   \n",
       "4    15.42  14.06  19.93  18.82  12.75  19.11   2.94   1.83  15.95  14.35   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "995   5.85   8.28   6.10  12.65   4.16   9.30   7.50   3.18   6.97  12.17   \n",
       "996   7.46  13.52  17.75  12.17   5.47   7.80   7.55  14.40  13.91   2.55   \n",
       "997  10.56  10.13  15.22  11.87  13.28  15.51   4.01   7.75  10.08  12.31   \n",
       "998   6.63   4.45   3.77   6.83   3.86   3.86   3.67   3.67   3.67   3.67   \n",
       "999  11.44  15.66  12.12   9.30  17.55  12.70  12.02   6.19  12.07  16.29   \n",
       "\n",
       "        60     61     62     63     64     65     66     67     68     69  \\\n",
       "0    19.59  14.59   4.16   1.97  13.82   9.64   1.92  19.30  16.68   6.19   \n",
       "1     4.30   7.65   1.97  15.47  15.08   7.17  19.74  12.12  11.78  18.52   \n",
       "2    17.46  18.28   0.00   0.00  18.04  18.28   0.00  18.28  19.25   0.00   \n",
       "3    11.00   7.31   0.00   0.00  18.82  11.24   0.00  18.28   8.67   0.00   \n",
       "4    18.38  17.17  15.71   8.72  18.38  15.56  18.14  15.22  14.01  14.83   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "995  12.36  11.10   5.27   8.57   5.66  11.97   4.25  13.38  12.55   6.00   \n",
       "996  16.44  14.83   8.18  16.00  18.23   2.41   7.31  16.68  16.44  11.63   \n",
       "997  13.43  14.45  13.14   8.96  10.32  12.21   4.45  14.20  12.55  10.37   \n",
       "998   7.21   8.33   3.38   3.67   7.17   6.83   3.67   3.77   3.86   3.38   \n",
       "999  15.90  13.09  19.01  11.68  16.34  17.41  13.09  17.55  15.32  11.15   \n",
       "\n",
       "        70     71     72     73     74     75     76     77     78     79  \\\n",
       "0     0.00   0.00   0.00   0.00   0.00   0.00   0.00   1.58   0.00   0.00   \n",
       "1     6.00  13.77  19.30  18.77  18.33  17.21  18.72  19.98  19.64  19.20   \n",
       "2     0.00   0.00   0.00   0.00   0.00  19.93   0.00   0.00   0.00  20.08   \n",
       "3     0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "4     0.00   0.00   0.00   0.00   0.00   0.00  15.13   0.00   0.00   0.00   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "995  14.64  13.09   8.48   7.02   4.30   5.95   7.36   6.63   7.55   2.46   \n",
       "996   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   4.06   0.00   \n",
       "997   0.00  13.62   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "998   4.30   4.30   4.30   4.30   3.86   6.63   4.11   4.11   7.21   3.82   \n",
       "999   0.00   0.00   0.00   0.00   8.91   0.00  14.54   0.00   0.00   0.00   \n",
       "\n",
       "        80     81     82     83     84     85     86     87     88     89  \\\n",
       "0     0.00   3.28   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "1    14.93  15.85  15.85  17.07  19.98  15.51  10.95  14.69  15.56  11.58   \n",
       "2     0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  20.03   \n",
       "3     0.00   0.00   0.00   0.00  11.63   0.00   0.00   8.67   0.00   0.00   \n",
       "4    16.24  16.92  11.87  18.28  14.93  10.37  17.31  15.71  13.82  13.96   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "995   3.48   7.12  10.95  10.03   7.89   7.12  11.15   8.77  13.86  10.71   \n",
       "996   0.00   0.00   0.00   0.00   0.00   0.00  13.62   0.00   0.00   0.00   \n",
       "997   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "998   3.82   3.82   3.82   3.33   4.16   9.20   4.16   4.16   3.82   3.82   \n",
       "999   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "\n",
       "        90     91     92     93     94     95     96     97     98     99  \n",
       "0    13.82   0.00   0.00   0.00   0.00   0.00   5.37   0.00   0.00   0.00  \n",
       "1    13.82   6.05  10.71  18.86  10.81   8.86  14.06  11.34   6.68  12.07  \n",
       "2     0.00   0.00   0.00  20.08   0.00   0.00   0.00   0.00   0.00   0.00  \n",
       "3     0.00   0.00   0.00  11.53   0.00   0.00   0.00   0.00   0.00   0.00  \n",
       "4    16.19  16.58  15.27  16.19  16.73  12.55  14.11  17.55  12.80  12.60  \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "995   6.58   9.93  15.37   7.89  13.72   6.87  13.23   5.47  14.54  13.38  \n",
       "996   0.00   0.00   0.00   0.00   6.58   0.00   0.00   0.00   0.00   0.00  \n",
       "997   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  \n",
       "998   3.82   6.87   6.87   3.77   3.77   3.77   3.77   3.77   3.77   3.28  \n",
       "999   0.00   0.00   0.00   0.00  12.12   0.00   0.00   0.00   0.00   0.00  \n",
       "\n",
       "[1000 rows x 100 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = pd.read_csv('joke-ratings.csv', header=None)\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "944903c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local code from the KNNRecommender Module\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from KNNRecommender import pearsonSim, cosineSim, knn_search, knn_predict, test, recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4cc6c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1: (100,), User 2: (100,)\n"
     ]
    }
   ],
   "source": [
    "user_1 = rate[0]\n",
    "user_2 = rate[1]\n",
    "print('User 1: {}, User 2: {}'.format(user_1.shape,user_2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38dc55",
   "metadata": {},
   "source": [
    "### The following four tables show top 10 neighbors by using Pearson and Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a18fe2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1. Top 10 neighbors for User 1 by using Pearson Similarity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>0.0</td>\n",
       "      <td>755.00</td>\n",
       "      <td>267.00</td>\n",
       "      <td>593.00</td>\n",
       "      <td>509.00</td>\n",
       "      <td>955.00</td>\n",
       "      <td>850.0</td>\n",
       "      <td>982.0</td>\n",
       "      <td>924.0</td>\n",
       "      <td>477.00</td>\n",
       "      <td>470.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5      6      7      8   \\\n",
       "user  0.0  755.00  267.00  593.00  509.00  955.00  850.0  982.0  924.0   \n",
       "sim   1.0    0.83    0.83    0.81    0.81    0.81    0.8    0.8    0.8   \n",
       "\n",
       "          9       10  \n",
       "user  477.00  470.00  \n",
       "sim     0.79    0.79  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, sims = knn_search(user_1, rate, metric=pearsonSim)\n",
    "neighbors = pd.DataFrame(idx, columns=['user'])\n",
    "neighbors[\"sim\"] = sims[idx]\n",
    "print('Table 1. Top 10 neighbors for User 1 by using Pearson Similarity')\n",
    "neighbors.head(11).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "567f263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 2. Top 10 neighbors for User 1 by using Cosine Similarity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>0.0</td>\n",
       "      <td>955.0</td>\n",
       "      <td>699.00</td>\n",
       "      <td>755.00</td>\n",
       "      <td>587.00</td>\n",
       "      <td>214.00</td>\n",
       "      <td>850.00</td>\n",
       "      <td>477.00</td>\n",
       "      <td>906.00</td>\n",
       "      <td>362.00</td>\n",
       "      <td>642.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1       2       3       4       5       6       7       8   \\\n",
       "user  0.0  955.0  699.00  755.00  587.00  214.00  850.00  477.00  906.00   \n",
       "sim   1.0    0.9    0.89    0.89    0.89    0.89    0.88    0.88    0.88   \n",
       "\n",
       "          9       10  \n",
       "user  362.00  642.00  \n",
       "sim     0.88    0.88  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, sims = knn_search(user_1, rate, metric=cosineSim)\n",
    "neighbors = pd.DataFrame(idx, columns=['user'])\n",
    "print('Table 2. Top 10 neighbors for User 1 by using Cosine Similarity')\n",
    "neighbors[\"sim\"] = sims[idx]\n",
    "neighbors.head(11).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ddc10f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3. Top 10 neighbors for User 2 by using Pearson Similarity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>1.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>170.00</td>\n",
       "      <td>227.00</td>\n",
       "      <td>705.00</td>\n",
       "      <td>609.00</td>\n",
       "      <td>239.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>635.00</td>\n",
       "      <td>672.00</td>\n",
       "      <td>198.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1       2       3       4       5       6      7       8   \\\n",
       "user  1.0  173.0  170.00  227.00  705.00  609.00  239.00  13.00  635.00   \n",
       "sim   1.0    0.7    0.68    0.68    0.68    0.67    0.66   0.64    0.64   \n",
       "\n",
       "          9       10  \n",
       "user  672.00  198.00  \n",
       "sim     0.63    0.63  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, sims = knn_search(user_2, rate, metric=pearsonSim)\n",
    "neighbors = pd.DataFrame(idx, columns=['user'])\n",
    "neighbors[\"sim\"] = sims[idx]\n",
    "print('Table 3. Top 10 neighbors for User 2 by using Pearson Similarity')\n",
    "neighbors.head(11).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f44f8bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 4. Top 10 neighbors for User 2 by using Cosine Similarity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>1.0</td>\n",
       "      <td>173.00</td>\n",
       "      <td>804.00</td>\n",
       "      <td>609.00</td>\n",
       "      <td>611.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>680.00</td>\n",
       "      <td>977.00</td>\n",
       "      <td>317.00</td>\n",
       "      <td>834.00</td>\n",
       "      <td>207.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4      5       6       7       8   \\\n",
       "user  1.0  173.00  804.00  609.00  611.00  38.00  680.00  977.00  317.00   \n",
       "sim   1.0    0.95    0.94    0.94    0.94   0.94    0.94    0.94    0.94   \n",
       "\n",
       "          9       10  \n",
       "user  834.00  207.00  \n",
       "sim     0.94    0.94  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, sims = knn_search(user_2, rate, metric=cosineSim)\n",
    "neighbors = pd.DataFrame(idx, columns=['user'])\n",
    "neighbors[\"sim\"] = sims[idx]\n",
    "print('Table 4. Top 10 neighbors for User 2 by using Cosine Similarity')\n",
    "neighbors.head(11).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353cac12",
   "metadata": {},
   "source": [
    "### Use Knn_predict to predict the ratings for:  \n",
    "### user 1, joke 1, Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a939cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 1 on joke No.1 is 11.99 \n",
      "Similarity method: Pearson\n",
      "The joke is:  This couple had an excellent relationship going until one day he came home from work to find his girlfriend packing. He asked her why she was leaving him and she told him that she had heard awful things about him. \"What could they possibly have said to make you move out?\" \"They told me that you were a pedophile.\" He replied \"That's an awfully big word for a ten year old.\"\n"
     ]
    }
   ],
   "source": [
    "# Testing knn_predict with default pearsonSim as similarity metric\n",
    "# use joke index 1 \n",
    "item = 1 \n",
    "# use k = 20\n",
    "rating = knn_predict(user_1, item, rate, 20)\n",
    "print(\"\\nPredicted Rating for User 1 on joke No.{} is {} \\nSimilarity method: Pearson\\nThe joke is: \"\n",
    "      .format(item,rating.round(2)),joke[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a97347d",
   "metadata": {},
   "source": [
    "### user 1, joke 1, Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eaca26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 1 on joke No.1 is 14.08 \n",
      "Similarity method: Cosine\n",
      "The joke is:  This couple had an excellent relationship going until one day he came home from work to find his girlfriend packing. He asked her why she was leaving him and she told him that she had heard awful things about him. \"What could they possibly have said to make you move out?\" \"They told me that you were a pedophile.\" He replied \"That's an awfully big word for a ten year old.\"\n"
     ]
    }
   ],
   "source": [
    "rating = knn_predict(user_1, item, rate, 20, metric=cosineSim)\n",
    "print(\"\\nPredicted Rating for User 1 on joke No.{} is {} \\nSimilarity method: Cosine\\nThe joke is: \"\n",
    "      .format(item,rating.round(2)),joke[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea845d83",
   "metadata": {},
   "source": [
    "### user 2, joke 6, Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "703e0000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 2 on joke No.6 is 12.82 \n",
      "Similarity method: Pearson\n",
      "The joke is:  How many feminists does it take to screw in a light bulb?That's not funny.\n"
     ]
    }
   ],
   "source": [
    "item = 6 \n",
    "# use k = 20\n",
    "rating = knn_predict(user_2, item, rate, 20)\n",
    "\n",
    "print(\"\\nPredicted Rating for User 2 on joke No.{} is {} \\nSimilarity method: Pearson\\nThe joke is: \"\n",
    "      .format(item,rating.round(2)),joke[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe59e2",
   "metadata": {},
   "source": [
    "### user 2, joke 6, Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "759a7a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 2 on joke No.6 is 12.57 \n",
      "Similarity method: Cosine\n",
      "The joke is:  How many feminists does it take to screw in a light bulb?That's not funny.\n"
     ]
    }
   ],
   "source": [
    "rating = knn_predict(user_2, item, rate, 20, metric=cosineSim)\n",
    "print(\"\\nPredicted Rating for User 2 on joke No.{} is {} \\nSimilarity method: Cosine\\nThe joke is: \"\n",
    "      .format(item,rating.round(2)),joke[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28eb83f",
   "metadata": {},
   "source": [
    "### Report MAE for top 50 users as test users and 10 neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5afc8ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoloute Error for K = 10  :  3.956746571350521\n"
     ]
    }
   ],
   "source": [
    "MAE = np.array([]) # to keep computed MAE values for each value of k\n",
    "k = 10\n",
    "num_test_users = 50 # Top 50 users for testing in this case\n",
    "test_ratio = 0.2  # ratio of rated items for each user used in testing\n",
    "error = test(rate, num_test_users, test_ratio, k)\n",
    "MAE = np.append(MAE,[error])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ef917",
   "metadata": {},
   "source": [
    "### Use recommend to find top 3 recommendations for:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e63cb",
   "metadata": {},
   "source": [
    "### User 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36b12739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Recommendations for User 6 :\n",
      "\n",
      "Joke No.75, Predicted Rating is 16.8 \n",
      "Similarity method: Peason\n",
      "The joke is: There once was a man and a woman that both  got in  a terrible car wreck. Both of their vehicles  were completely destroyed buy fortunately no one  was   hurt.  In thankfulness the woman said to the man 'We are both okay so we should celebrate. I have   a  bottle of wine in my car let's open it.' So the woman got the bottle out of the car and  handed it to the man. The man took a really big drink and handed the woman the bottle. The  woman  closed the bottle and put it down. The man  asked  'Aren't you going to take a drink?' The woman cleverly replied 'No I think I'll  just  wait for the cops to get here.'\n",
      "\n",
      "Joke No.0, Predicted Rating is 14.95 \n",
      "Similarity method: Peason\n",
      "The joke is: A man visits the doctor. The doctor says \"I have bad news for you.You have cancer and Alzheimer's disease\". The man replies \"Well thank God I don't have cancer!\"\n",
      "\n",
      "Joke No.98, Predicted Rating is 14.87 \n",
      "Similarity method: Peason\n",
      "The joke is: A bus station is where a bus stops.A train station is where a train stops.On my desk I have a work station...\n"
     ]
    }
   ],
   "source": [
    "user = 6\n",
    "N = 3 # the top 3 recommendations\n",
    "k = 20\n",
    "preds, items = recommend(user,R,k,N)\n",
    "\n",
    "print(\"Top {} Recommendations for User\".format(N), user, \":\")\n",
    "for i in range(len(items)):\n",
    "    print()\n",
    "    print(\"Joke No.{}, Predicted Rating is {} \\nSimilarity method: Peason\\nThe joke is:\"\n",
    "          .format(items[i],preds[i].round(2)),joke[items[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b295e",
   "metadata": {},
   "source": [
    "### User 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08a47f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Recommendations for User 66 :\n",
      "\n",
      "Joke No.72, Predicted Rating is 17.5 \n",
      "Similarity method: Peason\n",
      "The joke is: Q: What is the difference between George  Washington Richard Nixon and Bill Clinton? A: Washington couldn't tell a lie Nixon couldn't   tell the truth andClinton doesn't know the difference.\n",
      "\n",
      "Joke No.77, Predicted Rating is 17.48 \n",
      "Similarity method: Peason\n",
      "The joke is: Q: What's the difference between the government  and  the Mafia? A: One of them is organized.\n",
      "\n",
      "Joke No.88, Predicted Rating is 15.99 \n",
      "Similarity method: Peason\n",
      "The joke is: A radio conversation of a US naval ship with Canadian authorities ... Americans: Please divert your course 15 degrees to the North to avoid a collision.Canadians: Recommend you divert YOUR course 15 degrees to the South to avoid a collision.Americans: This is the Captain of a US Navy ship.  I say again divert YOUR course.Canadians: No.  I say again you divert YOUR course.Americans: This is the aircraft carrier USS LINCOLN the second largest ship in the United States' Atlantic Fleet. We are accompanied by three destroyers three cruisers and numerous support vessels. I demand that you change your course 15 degrees north that's ONE FIVE DEGREES NORTH or counter-measures will be undertaken to ensure the safety of this ship.Canadians: This is a lighthouse.  Your call.\n"
     ]
    }
   ],
   "source": [
    "user = 66\n",
    "N = 3 # the top 3 recommendations\n",
    "k = 20\n",
    "preds, items = recommend(user,R,k,N)\n",
    "\n",
    "print(\"Top {} Recommendations for User\".format(N), user, \":\")\n",
    "for i in range(len(items)):\n",
    "    print()\n",
    "    print(\"Joke No.{}, Predicted Rating is {} \\nSimilarity method: Peason\\nThe joke is:\"\n",
    "          .format(items[i],preds[i].round(2)),joke[items[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdb436",
   "metadata": {},
   "source": [
    "## 1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f55c6",
   "metadata": {},
   "source": [
    "Write a function \"user_sim_matrix\" that takes the ratings matrix and a similarity metric as input and returns a user-user similarity matrix (be sure to test your function with a small toy data set, such as this one, before testing on the full joke ratings matrix). Note: your pairwise similarities should be computed only on overlapping items between two users (items they have both rated). Then, modify the \"knn_predict\" function takes as input the pre-computed user similarity matrix (instead of calling \"knn_search\"), along with the ratings matrix, that target user, the target item, and the number of neighbors (k); and return the predicted rating as before. Test your new predict functions on several user-item pairs and with k=30 to demonstrate that it works (user Pearson similarity when computing similarity matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4398c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from numpy import linalg as la\n",
    "# Define similarity metric\n",
    "def euclidSim(inA,inB):\n",
    "    return 1.0 / (1.0 + la.norm(inA - inB))\n",
    "\n",
    "def pearsonSim(inA,inB):\n",
    "    if len(inA) < 3 : return 1.0\n",
    "    return 0.5 + 0.5 * corrcoef(inA, inB, rowvar = 0)[0][1]\n",
    "\n",
    "def cosineSim(inA,inB):\n",
    "    num = np.dot(inA,inB)\n",
    "    denom = la.norm(inA)*la.norm(inB)\n",
    "    return 0.5 + 0.5 * (num / denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd5a6e",
   "metadata": {},
   "source": [
    "<font size =4>*The function below accept rating np array and one of similarity metric as input and returns users' similarity matrix as np array*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66ce830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_sim_matrix(rate, sim_mtrc):\n",
    "    # get total user numebr \n",
    "    usr = np.shape(rate)[0]\n",
    "    # get total item number\n",
    "    item = np.shape(rate)[1]\n",
    "    # create a diagonal matrix for user similarity\n",
    "    user_sim = np.eye(usr)\n",
    "    \n",
    "    for i in range(usr):\n",
    "        # for user pair i, j caculate similarity \n",
    "        for j in range(i):\n",
    "            overLap = np.array([])\n",
    "            # Find all the item index that user i and user j both give the rating \n",
    "            overLap = np.nonzero(np.logical_and(rate[i,:] >0, \\\n",
    "                                      rate[j,:]>0))[0]\n",
    "            index = overLap.tolist()\n",
    "            # Calculate the similarity for user pair i, j\n",
    "            similarity = sim_mtrc(rate[i, index], rate[j, index])\n",
    "            # Put it into the right position \n",
    "            user_sim[i,j] = similarity\n",
    "            user_sim[j,i] = similarity\n",
    "            \n",
    "    return user_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f61af88",
   "metadata": {},
   "source": [
    "<font size =4>*Use test matrix to test the function below:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "475324e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "td =  np.loadtxt('test-matrix.csv', delimiter =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db1b4df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.91902624, 0.91030497, 0.88888889, 0.7037037 ],\n",
       "       [0.91902624, 1.        , 0.96494781, 0.92163702, 0.755655  ],\n",
       "       [0.91030497, 0.96494781, 1.        , 0.94342203, 0.85355339],\n",
       "       [0.88888889, 0.92163702, 0.94342203, 1.        , 0.70211302],\n",
       "       [0.7037037 , 0.755655  , 0.85355339, 0.70211302, 1.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_sim_matrix(td,cosineSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efedd81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daweiwang/miniforge3/lib/python3.9/site-packages/numpy/lib/function_base.py:2829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/daweiwang/miniforge3/lib/python3.9/site-packages/numpy/lib/function_base.py:2830: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 7.50000000e-01, 7.50000000e-01,            nan,\n",
       "        2.50000000e-01],\n",
       "       [7.50000000e-01, 1.00000000e+00, 9.85362672e-01,            nan,\n",
       "        1.46373283e-02],\n",
       "       [7.50000000e-01, 9.85362672e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [           nan,            nan, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.11022302e-16],\n",
       "       [2.50000000e-01, 1.46373283e-02, 0.00000000e+00, 1.11022302e-16,\n",
       "        1.00000000e+00]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_sim_matrix(td,pearsonSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2043a8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.1951941 , 0.25      , 0.2       , 0.1502211 ],\n",
       "       [0.1951941 , 1.        , 0.30901699, 0.1951941 , 0.13507811],\n",
       "       [0.25      , 0.30901699, 1.        , 0.28989795, 0.16396078],\n",
       "       [0.2       , 0.1951941 , 0.28989795, 1.        , 0.12613198],\n",
       "       [0.1502211 , 0.13507811, 0.16396078, 0.12613198, 1.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_sim_matrix(td,euclidSim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c486a8bb",
   "metadata": {},
   "source": [
    "<font size = 4>*Modified Knn predict function:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ad59852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(user_sim, tar_user, tar_item, RatingsMat, K):\n",
    "\n",
    "    \"\"\" Given user similarity matrix (a Numpy array), target user and item index  \"\"\"\n",
    "    \"\"\" RatingsMat, the ratings matrix (a 2d Numpy array), find the K nearest     \"\"\"\n",
    "    \"\"\" neighbors of user and use weighted average of their ratings on item as    \"\"\"\n",
    "    \"\"\" predicted rating for (user, item).    \"\"\"\n",
    "    \n",
    "    # for the target user, get indexes of neighbors in decreasing order by users' similarity\n",
    "    neigh_idx = user_sim[:,tar_user].argsort()[::-1] \n",
    "    # get the similarity value of the between users and the target user \n",
    "    sims = user_sim[:,tar_user]\n",
    "\n",
    "    # Ratings of the neighbors on the target item \n",
    "    # For the given item, get item's ratings from all users, decreasing order by similarity of users \n",
    "    neigh_ratings = RatingsMat[neigh_idx][:,tar_item]\n",
    "    # Users' similarity, decreasing order\n",
    "    neigh_sims = sims[neigh_idx]   \n",
    "    wr = 0\n",
    "    sum_sim = 0\n",
    "    \n",
    "    # Compute the weighted average rating of the neighbors on item\n",
    "    # Note here, the neigh_ratings and neigh_sim are all started with the target user itself (i=0)\n",
    "    # Here we start i with 1 and end i with K+1\n",
    "    for i in range(1,K+1):\n",
    "        if (neigh_ratings[i] > 0) & (neigh_sims[i] > 0):\n",
    "            \n",
    "            wr += neigh_ratings[i] * neigh_sims[i]\n",
    "            sum_sim += neigh_sims[i]\n",
    "    if sum_sim > 0:\n",
    "        predicted_rating = wr/sum_sim\n",
    "    else:\n",
    "        item_vec = RatingsMat[:,item]  # if there are no neighbors with ratings for item,then use\n",
    "                                       # the item's average rating across all users as the prediction\n",
    "        predicted_rating = (RatingsMat[:,item][item_vec > 0]).mean()\n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35066595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9852547562099447"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function\n",
    "usr_sim = user_sim_matrix(td,pearsonSim)\n",
    "# for user 2, item 2, k neighbor = 2\n",
    "knn_predict(usr_sim, 2,2,td,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bcd6d4",
   "metadata": {},
   "source": [
    "<font size=4 >*Now compute user similarity matrix by using Pearson Similarity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c454736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_sim = user_sim_matrix(rate,pearsonSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bc9aa4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usr_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d8f827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.38002143 0.54814758 ... 0.68064788 0.57228191 0.65583649]\n",
      " [0.38002143 1.         0.45916366 ... 0.32109158 0.43838592 0.37087484]\n",
      " [0.54814758 0.45916366 1.         ... 0.5439094  0.48924609 0.65188738]\n",
      " ...\n",
      " [0.68064788 0.32109158 0.5439094  ... 1.         0.58058795 0.60808086]\n",
      " [0.57228191 0.43838592 0.48924609 ... 0.58058795 1.         0.58313651]\n",
      " [0.65583649 0.37087484 0.65188738 ... 0.60808086 0.58313651 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(usr_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bda5513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.880979277993504"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_predict(usr_sim, 1,1,rate,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eafff3",
   "metadata": {},
   "source": [
    "### Test with previous pairs: user 1 - item 1, and user 2 - item 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b721dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 1 on joke No.1 is 12.88 \n",
      "Similarity method: Pearson\n",
      "K neighbors = 30\n",
      "The joke is:  This couple had an excellent relationship going until one day he came home from work to find his girlfriend packing. He asked her why she was leaving him and she told him that she had heard awful things about him. \"What could they possibly have said to make you move out?\" \"They told me that you were a pedophile.\" He replied \"That's an awfully big word for a ten year old.\"\n"
     ]
    }
   ],
   "source": [
    "user = 1\n",
    "item = 1\n",
    "K = 30\n",
    "rating = knn_predict(usr_sim, user,item,rate, K)\n",
    "print(\"\\nPredicted Rating for User 1 on joke No.{} is {} \\nSimilarity method: Pearson\\nK neighbors = {}\\nThe joke is: \"\n",
    "      .format(item,rating.round(2),K),joke[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92c126d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 2 on joke No.6 is 14.06 \n",
      "Similarity method: Pearson\n",
      "K neighbors = 30\n",
      "The joke is:  How many feminists does it take to screw in a light bulb?That's not funny.\n"
     ]
    }
   ],
   "source": [
    "user = 2\n",
    "item = 6\n",
    "rating = knn_predict(usr_sim, user,item,rate,30)\n",
    "print(\"\\nPredicted Rating for User 2 on joke No.{} is {} \\nSimilarity method: Pearson\\nK neighbors = {}\\nThe joke is: \"\n",
    "      .format(item,rating.round(2),K),joke[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7919578",
   "metadata": {},
   "source": [
    "### More testings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38634854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 888 on joke No.66 is 9.81 \n",
      "Similarity method: Pearson\n",
      "K neighbors = 30\n",
      "The joke is:  Once upon a time two brooms fell in love and decided to get married.Before the ceremony the bride broom informed the groom broom that she was expecting a little whiskbroom. The groom broom was aghast!\"How is this possible?\" he asked. \"We've never swept together!\n"
     ]
    }
   ],
   "source": [
    "user = 888\n",
    "item = 66\n",
    "rating = knn_predict(usr_sim,user,item,rate,30)\n",
    "print(\"\\nPredicted Rating for User {} on joke No.{} is {} \\nSimilarity method: Pearson\\nK neighbors = {}\\nThe joke is: \"\n",
    "      .format(user,item,rating.round(2),K),joke[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ad8b841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 222 on joke No.88 is 11.77 \n",
      "Similarity method: Pearson\n",
      "K neighbors = 30\n",
      "The joke is:  A radio conversation of a US naval ship with Canadian authorities ... Americans: Please divert your course 15 degrees to the North to avoid a collision.Canadians: Recommend you divert YOUR course 15 degrees to the South to avoid a collision.Americans: This is the Captain of a US Navy ship.  I say again divert YOUR course.Canadians: No.  I say again you divert YOUR course.Americans: This is the aircraft carrier USS LINCOLN the second largest ship in the United States' Atlantic Fleet. We are accompanied by three destroyers three cruisers and numerous support vessels. I demand that you change your course 15 degrees north that's ONE FIVE DEGREES NORTH or counter-measures will be undertaken to ensure the safety of this ship.Canadians: This is a lighthouse.  Your call.\n"
     ]
    }
   ],
   "source": [
    "user = 222\n",
    "item = 88\n",
    "rating = knn_predict(usr_sim,user,item,rate,30)\n",
    "print(\"\\nPredicted Rating for User {} on joke No.{} is {} \\nSimilarity method: Pearson\\nK neighbors = {}\\nThe joke is: \"\n",
    "      .format(user,item,rating.round(2),K),joke[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d7070",
   "metadata": {},
   "source": [
    "<font size =4> As we can see above:    \n",
    "    \n",
    "<font size =4>1. **user_sim_matrix** function intakes ratings matrix and work on each user. This function first build up a diagnoal matrix with 1 filled in the diagonal elements. Then for each user pair (i,j), this function finds all the item index that user i and user j both give the rating. Finally, the compute similarity by user's selection for each user pair (i,j) and store it in the matrix.  \n",
    "    \n",
    "*After testing on test cases, the function could give the same similarity as we maunally calculated, for the same given conditions.*\n",
    "      \n",
    "<font size =4>2. **Modified knn_predict** function then take the similarity matrix produced from **user_sim_matrix**, as well as target user, item and rating matrix. To compute the weighted average rating of the neighbors on item, the ratings of the neighbors on the target item, and the similarity value of the target user in decreasing order was created. Then the function calculate and return the predicted rating for this target user at the selected item.\n",
    "    \n",
    "    \n",
    "*After testing on test cases, the function could give the same similarity as we maunally calculated, for the same given conditions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922fa2fc",
   "metadata": {},
   "source": [
    "## 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef2df5",
   "metadata": {},
   "source": [
    "Create a version of an item-based KNN recommender. You can modify the user-based prediction function from the previous part that instead uses and item-item similarity matrix (you'll also need to create a new version of the function for the similarity matrix that compute pairwise similarities among items). The predicted ratings would be computed as the weighted average of the ratings of item neighbors to the target item that have been rated by the target user (be sure to review class notes describing item-based KNN model). Test your item-based predict function as in part b, but this time use Cosine similarity for computing item-item similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e47a211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_sim_matrix(rate, sim_mtrc):\n",
    "    # get total user numebr \n",
    "    usr = np.shape(rate)[0]\n",
    "    # get total item number\n",
    "    item = np.shape(rate)[1]\n",
    "    # create a diagonal matrix for user similarity\n",
    "    item_sim = np.eye(item)\n",
    "    \n",
    "    for i in range(item):\n",
    "        # for item pair i,j caculate similarity \n",
    "        for j in range(i):\n",
    "            overLap = np.array([])\n",
    "            # Find all the user index that item i and item j both have the rating \n",
    "            overLap = np.nonzero(np.logical_and(rate[:,i] >0, \\\n",
    "                                      rate[:,j]>0))[0]\n",
    "            index = overLap.tolist()\n",
    "            # Calculate the similarity for item pair i, j\n",
    "            similarity = sim_mtrc(rate[index,i], rate[index,j])\n",
    "            # Put it into the right position \n",
    "            item_sim[i,j] = similarity\n",
    "            item_sim[j,i] = similarity\n",
    "            \n",
    "    return item_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d21c33",
   "metadata": {},
   "source": [
    "*Test this function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6104ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix with 6 users and 5 items \n",
    "ty = array([[0., 5., 1., 1., 1.],\n",
    "            [1., 5., 5., 0., 2.],\n",
    "            [2., 3., 0., 3., 2.],\n",
    "            [1., 0., 1., 5., 1.],\n",
    "            [5., 1., 0., 1., 5.],\n",
    "            [2., 0., 3., 1., 5.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce5db198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.74688536, 0.91403934, 0.75724788, 0.96212479],\n",
       "       [0.74688536, 1.        , 0.91602515, 0.88223539, 0.78782484],\n",
       "       [0.91403934, 0.91602515, 1.        , 0.76111648, 0.90411193],\n",
       "       [0.75724788, 0.88223539, 0.76111648, 1.        , 0.74165613],\n",
       "       [0.96212479, 0.78782484, 0.90411193, 0.74165613, 1.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = item_sim_matrix(ty, cosineSim)\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9f5bc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_based_knn_predict(item_sim, tar_user, tar_item, RatingsMat, K):\n",
    "\n",
    "    \"\"\" Given item similarity matrix (a Numpy array), target user and item index  \"\"\"\n",
    "    \"\"\" RatingsMat, the ratings matrix (a 2d Numpy array), find the K nearest     \"\"\"\n",
    "    \"\"\" neighbors of user and use weighted average of their ratings on item as    \"\"\"\n",
    "    \"\"\" predicted rating for (user, item).    \"\"\"\n",
    "    \n",
    "    # for the target item, get indexes of neighbors in decreasing order \n",
    "    neigh_idx = item_sim[:,tar_item].argsort()[::-1] \n",
    "    # get the similarity value of the target item\n",
    "    sims = item_sim[:,tar_item]\n",
    "\n",
    "    # Ratings of the neighbors on the target user \n",
    "    # For the given user, get user's ratings from all items, decreasing order by similarity of items \n",
    "    neigh_ratings = RatingsMat[tar_user][neigh_idx]\n",
    "    # Items' similarity, decreasing order\n",
    "    neigh_sims = sims[neigh_idx]   \n",
    "    wr = 0\n",
    "    sum_sim = 0\n",
    "    \n",
    "    # compute the weighted average rating of the neighbors on item\n",
    "    for i in range(1,K+1):\n",
    "        if (neigh_ratings[i] > 0) & (neigh_sims[i] > 0):\n",
    "            \n",
    "            wr += neigh_ratings[i] * neigh_sims[i]\n",
    "            sum_sim += neigh_sims[i]\n",
    "    if sum_sim > 0:\n",
    "        predicted_rating = wr/sum_sim\n",
    "    else:\n",
    "        user_vec = RatingsMat[user]  # if there are no neighbors with ratings for item,then use \n",
    "                                       # the item's average rating across all users as the prediction\n",
    "        predicted_rating = (RatingsMat[user][item_vec > 0]).mean()\n",
    "    return predicted_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecfe73a",
   "metadata": {},
   "source": [
    "*Test on small dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "53480a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0130904619888152"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_based_knn_predict(sim, 0, 2, ty, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6aa3a8",
   "metadata": {},
   "source": [
    "<font size=4 >*Now compute user similarity matrix by using Cosine Similarity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09c4bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sim = item_sim_matrix(rate,cosineSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23033b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab1ee345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.9432675  0.9445507  ... 0.92365665 0.93388791 0.92730944]\n",
      " [0.9432675  1.         0.92944184 ... 0.92857529 0.92266094 0.93446599]\n",
      " [0.9445507  0.92944184 1.         ... 0.91696385 0.91407451 0.93087925]\n",
      " ...\n",
      " [0.92365665 0.92857529 0.91696385 ... 1.         0.93338147 0.9504037 ]\n",
      " [0.93388791 0.92266094 0.91407451 ... 0.93338147 1.         0.92871448]\n",
      " [0.92730944 0.93446599 0.93087925 ... 0.9504037  0.92871448 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(item_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f6c87",
   "metadata": {},
   "source": [
    "### Test with previous pairs: user 1 - item 1, and user 2 - item 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fff7028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 1 on joke No.1 is 13.03 \n",
      "Similarity method: Cosine\n",
      "K neighbors = 30\n",
      "The joke is:  This couple had an excellent relationship going until one day he came home from work to find his girlfriend packing. He asked her why she was leaving him and she told him that she had heard awful things about him. \"What could they possibly have said to make you move out?\" \"They told me that you were a pedophile.\" He replied \"That's an awfully big word for a ten year old.\"\n"
     ]
    }
   ],
   "source": [
    "user = 1\n",
    "item = 1\n",
    "K = 30\n",
    "rating = item_based_knn_predict(item_sim, user, item, rate, K)\n",
    "print(\"\\nPredicted Rating for User {} on joke No.{} is {} \\nSimilarity method: Cosine\\nK neighbors = {}\\nThe joke is: \"\n",
    "      .format(user,item,rating.round(2),K),joke[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e33ce07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 2 on joke No.6 is 18.71 \n",
      "Similarity method: Cosine\n",
      "K neighbors = 30\n",
      "The joke is:  How many feminists does it take to screw in a light bulb?That's not funny.\n"
     ]
    }
   ],
   "source": [
    "user = 2\n",
    "item = 6\n",
    "K = 30\n",
    "rating = item_based_knn_predict(item_sim, user, item, rate, K)\n",
    "print(\"\\nPredicted Rating for User {} on joke No.{} is {} \\nSimilarity method: Cosine\\nK neighbors = {}\\nThe joke is: \"\n",
    "      .format(user,item,rating.round(2),K),joke[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47916583",
   "metadata": {},
   "source": [
    "### More random tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8a8dd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 3 on joke No.8 is 13.06 \n",
      "Similarity method: Cosine\n",
      "K neighbors = 20\n",
      "The joke is:  A country guy goes into a city bar that has a dress code and the maitred' demands he wear a tie. Discouraged the guy goes to his car to sulk when inspiration strikes: He's got jumper cables in the trunk! So he wraps them around his neck sort of like a string tie (a bulky string tie to be sure) and returns to the bar. The maitre d' is reluctant but says to the guy \"Okay you're a pretty resourceful fellow you can come in... but just don't start anything\"!\n"
     ]
    }
   ],
   "source": [
    "user = 3\n",
    "item = 8\n",
    "K = 20\n",
    "rating = item_based_knn_predict(item_sim, user, item, rate, K)\n",
    "print(\"\\nPredicted Rating for User {} on joke No.{} is {} \\nSimilarity method: Cosine\\nK neighbors = {}\\nThe joke is: \"\n",
    "      .format(user,item,rating.round(2),K),joke[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d95a6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 6 on joke No.15 is 15.9 \n",
      "Similarity method: Cosine\n",
      "K neighbors = 22\n",
      "The joke is:  Q. What is orange and sounds like a parrot?  A. A carrot.\n"
     ]
    }
   ],
   "source": [
    "user = 6\n",
    "item = 15\n",
    "K = 22\n",
    "rating = item_based_knn_predict(item_sim, user, item, rate, K)\n",
    "print(\"\\nPredicted Rating for User {} on joke No.{} is {} \\nSimilarity method: Cosine\\nK neighbors = {}\\nThe joke is: \"\n",
    "      .format(user,item,rating.round(2),K),joke[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3dae115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Rating for User 5 on joke No.77 is 6.68 \n",
      "Similarity method: Cosine\n",
      "K neighbors = 22\n",
      "The joke is:  Q: What's the difference between the government  and  the Mafia? A: One of them is organized.\n"
     ]
    }
   ],
   "source": [
    "user = 5\n",
    "item = 77\n",
    "K = 22\n",
    "rating = item_based_knn_predict(item_sim, user, item, rate, K)\n",
    "print(\"\\nPredicted Rating for User {} on joke No.{} is {} \\nSimilarity method: Cosine\\nK neighbors = {}\\nThe joke is: \"\n",
    "      .format(user,item,rating.round(2),K),joke[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1dcee",
   "metadata": {},
   "source": [
    "<font size =4> Just like the function in part b:    \n",
    "    \n",
    "<font size =4>1. **item_sim_matrix** function intakes ratings matrix and work on each item. This function computes similarity for each item pair (i,j) and store it in the matrix.  \n",
    "    \n",
    "*After testing on test cases, the function could give the same similarity as we maunally calculated, for the same given conditions.*\n",
    "      \n",
    "<font size =4>2. **Modified knn_predict** then take the similarity matrix produced from **item_sim_matrix**. The difference here is this function gets ratings from the target user and calculate predictions for a certain item.\n",
    "    \n",
    "    \n",
    "*After testing on test cases, the function could give the same similarity as we maunally calculated, for the same given conditions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c856dbe",
   "metadata": {},
   "source": [
    "## 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4e20f",
   "metadata": {},
   "source": [
    "Modify the \"cross_validate_user\" and the \"test\" functions to use the new item-based predict function from the previous part. Note that the only modification that should be necessary are changing the signature of these functions (to take a similarity matrix as input instead of a similarity metric) and the call to \"knn_predict\" (in \"cross_validate_user\") and the call to \"cross_validate_user\" (in \"test\"). The rest of the code should not need modification. Test your \"test\" function as in part (a) to make sure it works. Then, run the test on the full ratings matrix with 0.2 test ratio for values of k from 1 to 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eddab88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate user by using item based predict function in 1c\n",
    "\n",
    "def cross_validate_user(dataMat, item_sim, user, test_ratio, K):\n",
    "\n",
    "    \"\"\" For a given test user, this function randomly selects test_ratio percent of the  \"\"\"\n",
    "    \"\"\" already rated items and computes the prediction errors for these test items      \"\"\"\n",
    "\n",
    "    number_of_items = np.shape(dataMat)[1]\n",
    "    rated_items_by_user = np.array([i for i in range(number_of_items) if dataMat[user,i]>0])\n",
    "    test_size = int(test_ratio * len(rated_items_by_user))\n",
    "    test_indices = np.random.randint(0, len(rated_items_by_user), test_size)\n",
    "    withheld_items = rated_items_by_user[test_indices]\n",
    "    original_user_profile = np.copy(dataMat[user]) # maintain the original ratings to be restored later\n",
    "    dataMat[user, withheld_items] = 0 # So that the withheld test items is not used in the rating prediction below\n",
    "    error_u = 0.0\n",
    "    count_u = len(withheld_items)\n",
    "\n",
    "    # Compute absolute error for user u over all test items\n",
    "    for item in withheld_items:\n",
    "        # Estimate rating on the withheld item\n",
    "        u = dataMat[user]\n",
    "        # print(\"user: \", u, \"Item: \", i)\n",
    "        predicted_rating = item_based_knn_predict(item_sim, user, item, dataMat, K)\n",
    "        error_u = error_u + abs(predicted_rating - original_user_profile[item])\n",
    "\n",
    "    # Now restore ratings of the withheld items to the user profile\n",
    "    for item in withheld_items:\n",
    "        dataMat[user, item] = original_user_profile[item]\n",
    "\n",
    "    # Return sum of absolute errors and the count of test cases for this user\n",
    "    # Note that these will have to be accumulated for each user to compute MAE\n",
    "    return error_u, count_u\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "686fb083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test by using item based predict function in 1c\n",
    "def test(dataMat, item_sim, num_test_users, test_ratio, K):\n",
    "\n",
    "    \"\"\" This function performs cross_validate_user on the first num_test_users in the training data \"\"\"\n",
    "    \"\"\" It returns the Mean Absolute Error (MAE) across all test cases. \"\"\"\n",
    "\n",
    "    total_error=0.0;\n",
    "    total_test_cases=0.0\n",
    "    for u in range(num_test_users):\n",
    "        error_u, count = cross_validate_user(dataMat,item_sim, u , test_ratio, K)\n",
    "        # print('Evaluating user', u, ' out of', num_test_users, 'MAE: ', error_u/count)\n",
    "        total_error=total_error+error_u\n",
    "        total_test_cases=total_test_cases+count\n",
    "    print('Mean Absoloute Error for K =',K,' : ', total_error/total_test_cases)\n",
    "    return(total_error/total_test_cases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "372809fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoloute Error for K = 1  :  4.115201188490936\n",
      "Mean Absoloute Error for K = 2  :  3.8779880577858012\n",
      "Mean Absoloute Error for K = 3  :  3.7334049789222417\n",
      "Mean Absoloute Error for K = 4  :  3.650936660939383\n",
      "Mean Absoloute Error for K = 5  :  3.5687017818841267\n",
      "Mean Absoloute Error for K = 6  :  3.615728839157506\n",
      "Mean Absoloute Error for K = 7  :  3.5750658699647495\n",
      "Mean Absoloute Error for K = 8  :  3.6006040923295735\n",
      "Mean Absoloute Error for K = 9  :  3.524720569119608\n",
      "Mean Absoloute Error for K = 10  :  3.54903951320887\n",
      "Mean Absoloute Error for K = 11  :  3.5283371026541626\n",
      "Mean Absoloute Error for K = 12  :  3.544170960284583\n",
      "Mean Absoloute Error for K = 13  :  3.5096850452242006\n",
      "Mean Absoloute Error for K = 14  :  3.504428674778672\n",
      "Mean Absoloute Error for K = 15  :  3.5854274382489493\n",
      "Mean Absoloute Error for K = 16  :  3.5348020740517776\n",
      "Mean Absoloute Error for K = 17  :  3.533003436050666\n",
      "Mean Absoloute Error for K = 18  :  3.540342568932531\n",
      "Mean Absoloute Error for K = 19  :  3.507447034215547\n",
      "Mean Absoloute Error for K = 20  :  3.567242077635087\n",
      "Mean Absoloute Error for K = 21  :  3.5332941014757178\n",
      "Mean Absoloute Error for K = 22  :  3.53470475903043\n",
      "Mean Absoloute Error for K = 23  :  3.562069314984398\n",
      "Mean Absoloute Error for K = 24  :  3.593095475906422\n",
      "Mean Absoloute Error for K = 25  :  3.5605211277211044\n",
      "Mean Absoloute Error for K = 26  :  3.5605197260115\n",
      "Mean Absoloute Error for K = 27  :  3.534144144004499\n",
      "Mean Absoloute Error for K = 28  :  3.550965349295842\n",
      "Mean Absoloute Error for K = 29  :  3.582413905871737\n",
      "Mean Absoloute Error for K = 30  :  3.5432688007362074\n",
      "Mean Absoloute Error for K = 31  :  3.606925022343077\n",
      "Mean Absoloute Error for K = 32  :  3.535857335332767\n",
      "Mean Absoloute Error for K = 33  :  3.6055910371340545\n",
      "Mean Absoloute Error for K = 34  :  3.5619205637655513\n",
      "Mean Absoloute Error for K = 35  :  3.550647894020954\n",
      "Mean Absoloute Error for K = 36  :  3.561721671002742\n",
      "Mean Absoloute Error for K = 37  :  3.593389141239017\n",
      "Mean Absoloute Error for K = 38  :  3.5828462030773403\n",
      "Mean Absoloute Error for K = 39  :  3.5854716833068685\n",
      "Mean Absoloute Error for K = 40  :  3.5349342067844503\n"
     ]
    }
   ],
   "source": [
    "MAE = np.array([]) \n",
    "test_ratio = 0.2\n",
    "num_test_users = 1000\n",
    "for k in range(1,41):\n",
    "    error = test(rate, item_sim, num_test_users, test_ratio, k)\n",
    "    MAE = np.append(MAE,[error])\n",
    "    \n",
    "k = list(range(1,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3f7cf0c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'k value'),\n",
       " Text(0, 0.5, 'MAE'),\n",
       " Text(0.5, 1.0, 'MAE under different k value')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc = {'figure.figsize':(8,4)})\n",
    "sns.lineplot(x = k,y = MAE).set(xlabel='k value', ylabel = 'MAE', title = 'MAE under different k value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfea39",
   "metadata": {},
   "source": [
    "## 1e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676fde1d",
   "metadata": {},
   "source": [
    "Use the best k value (the one resulting in lowest MAE) from part (d) to generate top 3 recommendations for at least two users. Note that you'll need a slightly modified \"recommend\" function that would use the similarity matrix and would call your new item-based \"predict\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96ff33dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_best = np.where(MAE == MAE.min())[0][0]\n",
    "k_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "78ac6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(item_sim, user, Ratings, K, N=3):\n",
    "\n",
    "    \"\"\" Given user (integer row index) and Ratings (a Pandas DataFrame), find the top N \"\"\"\n",
    "    \"\"\" recommended items for user (unrated items with highest predicted rating)        \"\"\"\n",
    "\n",
    "    u = np.array(Ratings.iloc[user])\n",
    "    RatingsMat = np.array(Ratings)\n",
    "    predictions = np.zeros(len(u))\n",
    "    unrated = 0\n",
    "\n",
    "    for j in range(len(u)):\n",
    "        if u[j] == 0:\n",
    "            unrated += 1\n",
    "            j_pred = item_based_knn_predict(item_sim, user, j, RatingsMat, K)\n",
    "            # print(j, j_pred)\n",
    "            predictions[j] = j_pred\n",
    "\n",
    "    recs = np.argsort(predictions)\n",
    "    recs = recs[::-1]\n",
    "    if unrated < N: N = unrated\n",
    "    preds = predictions[recs[:N]]\n",
    "    items = Ratings.columns[recs[:N]]\n",
    "\n",
    "    return preds, items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252931a",
   "metadata": {},
   "source": [
    "### Test user 6 and user 66 as 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9fc58e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Recommendations for User 6 :\n",
      "\n",
      "Joke No.42, Predicted Rating is 17.76 \n",
      "Similarity method: Peason\n",
      "The joke is: Arnold Swartzeneger and Sylvester Stallone are making a movie about the lives of the great composers.  Stallone says \"I want to be Mozart.\" Swartzeneger says: \"In that case...  I'll be Bach.\"\n",
      "\n",
      "Joke No.69, Predicted Rating is 17.49 \n",
      "Similarity method: Peason\n",
      "The joke is: Employer to applicant: \"In this job we need someone who is responsible.\"Applicant: \"I'm the one you want. On my last job every time anything went wrong they said I was responsible.\"\n",
      "\n",
      "Joke No.85, Predicted Rating is 17.35 \n",
      "Similarity method: Peason\n",
      "The joke is: A neutron walks into a bar and orders a drink.\"How much do I owe you?\" the neutron asks.The bartender replies \"for you no charge.\"\n"
     ]
    }
   ],
   "source": [
    "user = 6\n",
    "N = 3 # the top 3 recommendations\n",
    "preds, items = recommend(item_sim, user, R, k_best, N)\n",
    "\n",
    "print(\"Top {} Recommendations for User\".format(N), user, \":\")\n",
    "for i in range(len(items)):\n",
    "    print()\n",
    "    print(\"Joke No.{}, Predicted Rating is {} \\nSimilarity method: cosine\\nThe joke is:\"\n",
    "          .format(items[i],preds[i].round(2)),joke[items[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05a1dba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Recommendations for User 66 :\n",
      "\n",
      "Joke No.78, Predicted Rating is 12.69 \n",
      "Similarity method: cosine\n",
      "The joke is: Q: Ever wonder why the IRS calls it Form 1040?A: Because for every $50 that you earn you get 10 and they get 40.\n",
      "\n",
      "Joke No.76, Predicted Rating is 12.66 \n",
      "Similarity method: cosine\n",
      "The joke is: If pro- is the opposite of con- then congress must be the opposite of progress.\n",
      "\n",
      "Joke No.74, Predicted Rating is 12.44 \n",
      "Similarity method: cosine\n",
      "The joke is: Q: Do you know the difference between an intelligent male and theSasquatch? A: There have been actual reported sightings of the Sasquatch.\n"
     ]
    }
   ],
   "source": [
    "user = 66\n",
    "N = 3 \n",
    "preds, items = recommend(item_sim, user, R, k_best, N)\n",
    "\n",
    "print(\"Top {} Recommendations for User\".format(N), user, \":\")\n",
    "for i in range(len(items)):\n",
    "    print()\n",
    "    print(\"Joke No.{}, Predicted Rating is {} \\nSimilarity method: cosine\\nThe joke is:\"\n",
    "          .format(items[i],preds[i].round(2)),joke[items[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ba048",
   "metadata": {},
   "source": [
    "## 1f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac2113",
   "metadata": {},
   "source": [
    "One way to use item-item similarities for a \"localized\" version of recommendation is to generate \"similar\" items  to a given target item. \n",
    "\n",
    "Note that this is not personalized (does not use all of user's past ratings), but it can be used to generate Amazon-like \"people who like this item also liked ...\" recommendation for an item purchased or selected by the user. \n",
    "\n",
    "For this part, write a function that take an item (column index in the joke ratings matrix as before) and the pre-computed similarity matrix as inputs, and generates top N recommended jokes based on their similarities to the input query joke. \n",
    "\n",
    "Test your function for at least two different items and with N=3 as the number of recommendations (as before use the Cosine-based similarity matric). In the output, provide the text of the query joke as well as the text of the N most similar jokes given as recommendations along with their similarity values to the query joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "255e5e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 10, 11, 35, 68, 49, 34, 39, 47, 25,  0, 21, 44, 65, 55, 48, 51,\n",
       "       26, 31, 20, 38, 67, 24, 58, 61, 32, 60, 28,  9, 52, 13, 29, 41,  5,\n",
       "       69, 63, 86, 45, 64, 37, 53, 30, 33, 22, 27, 82, 88, 87,  3,  8, 92,\n",
       "       46, 91, 54,  4, 94, 50, 71, 77, 99, 18,  1, 36, 80, 95, 42, 23, 40,\n",
       "       75, 81, 62, 85, 83, 90, 59,  7, 19, 16, 66, 96, 17, 56, 84,  6, 93,\n",
       "       12, 72, 97, 79, 76, 98, 74, 14, 43, 73, 89, 70, 57, 78, 15])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh_idx = item_sim[:,2].argsort()[::-1] \n",
    "neigh_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3ee090e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95675322, 0.95411614, 0.94758296])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = item_sim[:,2]\n",
    "sims\n",
    "neigh_sims = sims[neigh_idx] \n",
    "neigh_sims[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7cba5bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_recommend(item_sim, tar_item, N):\n",
    "\n",
    "    \"\"\" Given item similarity matrix (a Numpy array), target item index and Number of \"\"\"\n",
    "    \"\"\" most similar items the user wish to have. This function will return top N     \"\"\"\n",
    "    \"\"\" recommend items' indexes and the similarity value   \"\"\"\n",
    "\n",
    "    # for the target item, get indexes of similarity items \n",
    "    neigh_idx = item_sim[:,tar_item].argsort()[::-1] \n",
    "    \n",
    "    # get the similarity value of the target item\n",
    "    sims = item_sim[:,tar_item]\n",
    "    \n",
    "    # Items' similarity, decreasing order\n",
    "    neigh_sims = sims[neigh_idx]   \n",
    "    \n",
    "    # prediction indexes, start with most similar \n",
    "    preds = neigh_idx[1:N+1]\n",
    "    # similarity \n",
    "    sim = neigh_sims[1:N+1]\n",
    "    \n",
    "    return preds, sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7849777c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query joke is:\n",
      "[Joke No.6]: How many feminists does it take to screw in a light bulb?That's not funny.\n",
      "\n",
      "Top 3 Recommendations are:\n",
      "\n",
      "Top No.1, Cosine similarity value is 0.9474 \n",
      "[Joke No.49]: A guy goes into confession and says to the priest \"Father I'm 80 years old widower with 11 grandchildren. Last night I met two beautiful flight attendants. They took me home and I made love to both of them. Twice.\"The priest said: \"Well my son when was the last time you were in confession?\" \"Never Father I'm Jewish.\" \"So then why are you telling me?\" \"I'm telling everybody.\"\n",
      "\n",
      "\n",
      "Top No.2, Cosine similarity value is 0.9463 \n",
      "[Joke No.35]: A guy walks into a bar orders a beer and says to the bartender\"Hey I got this great Polish Joke...\" The barkeep glares at him and says in a warning tone of voice:\"Before you go telling that joke you better know that I'm Polish both bouncers are Polish and so are most of my customers\"\"Okay\" says the customer\"I'll tell it very slowly.\"\n",
      "\n",
      "\n",
      "Top No.3, Cosine similarity value is 0.9446 \n",
      "[Joke No.28]: An old Scotsmen is sitting with a younger Scottish gentleman and says the boy. \"Ah lad look out that window. You see that stone wall there I built it with me own bare hands placed every stone meself.  But do they call me MacGregor the wall builder? No! He Takes a few sips of his beer then says \"Aye and look out on that lake and eye that beautiful pier. I built it meself laid every board and hammered each nail but do they call me MacGregor the pier builder? No! He continues...\"And lad you see that road? That too I build with me own bare hands. Laid every inch of pavement meself but do they call MacGregor the roadbuilder? No!\"Again he returns to his beer for a few sips then says \"Agh but you screw one sheep...\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tar_item = 6\n",
    "N = 3 \n",
    "preds, sim = local_recommend(item_sim, tar_item, N)\n",
    "\n",
    "print('The query joke is:\\n[Joke No.{}]: {}\\n'.format(tar_item,joke[tar_item]))\n",
    "print(\"Top {} Recommendations are:\".format(N))\n",
    "for i in range(len(sim)):\n",
    "    print()\n",
    "    print(\"Top No.{}, Cosine similarity value is {} \\n[Joke No.{}]: {}\\n\"\n",
    "          .format(i+1,sim[i].round(4),preds[i],joke[preds[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "edbc840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query joke is:\n",
      "[Joke No.66]: Once upon a time two brooms fell in love and decided to get married.Before the ceremony the bride broom informed the groom broom that she was expecting a little whiskbroom. The groom broom was aghast!\"How is this possible?\" he asked. \"We've never swept together!\n",
      "\n",
      "Top 3 Recommendations are:\n",
      "\n",
      "Top No.1, Cosine similarity value is 0.9493 \n",
      "[Joke No.54]: A woman has twins and gives them up for adoption.  One of them goes to a family in Egypt and is named \"Amal.\"  The other goes to a  family in Spain; they name him \"Juan.\"  Years later Juan sends a picture of himself to his mom.  Upon receiving the picture she tells her husband that she wishes she also had a picture of Amal.  Her husband responds \"But they are twins-if you've seen Juan you've seen   Amal.\n",
      "\n",
      "\n",
      "Top No.2, Cosine similarity value is 0.9466 \n",
      "[Joke No.8]: A country guy goes into a city bar that has a dress code and the maitred' demands he wear a tie. Discouraged the guy goes to his car to sulk when inspiration strikes: He's got jumper cables in the trunk! So he wraps them around his neck sort of like a string tie (a bulky string tie to be sure) and returns to the bar. The maitre d' is reluctant but says to the guy \"Okay you're a pretty resourceful fellow you can come in... but just don't start anything\"!\n",
      "\n",
      "\n",
      "Top No.3, Cosine similarity value is 0.9463 \n",
      "[Joke No.93]: Two atoms are walking down the street when one atom says to the other \"Oh my! I've lost an electron!\"The second atom says\"Are you sure\"The first replies \"I'm positive!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tar_item = 66\n",
    "N = 3 \n",
    "preds, sim = local_recommend(item_sim, tar_item, N)\n",
    "\n",
    "print('The query joke is:\\n[Joke No.{}]: {}\\n'.format(tar_item,joke[tar_item]))\n",
    "print(\"Top {} Recommendations are:\".format(N))\n",
    "for i in range(len(sim)):\n",
    "    print()\n",
    "    print(\"Top No.{}, Cosine similarity value is {} \\n[Joke No.{}]: {}\\n\"\n",
    "          .format(i+1,sim[i].round(4),preds[i],joke[preds[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae65b674",
   "metadata": {},
   "source": [
    "<font size = 4>As we can see above, the **local_recommend** function finds top N recommendations based on the similarity matrix. It will return the item indexes and similarity value in a decreasing order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ebf21",
   "metadata": {},
   "source": [
    "## Part 2. More Exploration of the Scikit Surpise Package for Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ace13d0",
   "metadata": {},
   "source": [
    "### 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af7943",
   "metadata": {},
   "source": [
    "Use the Surpise package with the joke ratings data set. Note that the data set used in part 1 is not in the correct (sparse) format used by Surprise. First, you need to convert this data into a dataframe with each row corresponding to a rating by a user on an item (with columns: \"user_id\", \"joke_id\", and \"rating\". See the example Jupyter Notebook linked above for an illustration. Be sure that the id fields and the rating field are of integer data type. As in previous assignment, you can use the Dataset.load_from_df function to load this data into Surprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "db960ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bcc382",
   "metadata": {},
   "source": [
    "<font size = 4>*- Convert rating dataset to the tidy table*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d3388537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id  rating\n",
       "0         0        0       3\n",
       "1         0        1      19\n",
       "2         0        2       1\n",
       "3         0        3       2\n",
       "4         0        4       3\n",
       "5         0        5       2\n",
       "6         0        6       1\n",
       "7         0        7      15\n",
       "8         0        8       2\n",
       "9         0        9       6\n",
       "10        0       10       2\n",
       "11        0       11       4\n",
       "12        0       12       3\n",
       "13        0       13      19\n",
       "14        0       14       3\n",
       "15        0       15       3\n",
       "16        0       16       3\n",
       "17        0       17       1\n",
       "18        0       18       1\n",
       "19        0       19       1"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.DataFrame([],columns=['user_id', 'item_id', 'rating'], dtype=int)\n",
    "\n",
    "for u in R.index:\n",
    "    for i in R.columns:\n",
    "        if R[i][u] > 0:\n",
    "            new_rating = {\"user_id\":u, \"item_id\":i, \"rating\":int(R[i][u])}\n",
    "            ratings = ratings.append(new_rating, ignore_index=True)\n",
    "\n",
    "ratings.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96686999",
   "metadata": {},
   "source": [
    "<font size = 4>*- Find out rating scale*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1cd612b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating Scale\n",
      "Max: 20\n",
      "Min: 1\n"
     ]
    }
   ],
   "source": [
    "print('Rating Scale')\n",
    "print('Max:',ratings.rating.max())\n",
    "print('Min:',ratings.rating.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b2537",
   "metadata": {},
   "source": [
    "<font size = 4>*- Create the dataset in surprise dataset type*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c190689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating_scale 1-20\n",
    "reader = Reader(rating_scale=(1, 20))\n",
    "# Load data \n",
    "data = Dataset.load_from_df(ratings[['user_id', 'item_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260ebb15",
   "metadata": {},
   "source": [
    "### 2b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc40e6b",
   "metadata": {},
   "source": [
    "Compare mean cross-validation MAEs for the following algorithms (using cross_validate function from surprise.model_selection):<br>&emsp;· SlopeOne<br>&emsp;· KNNWithMeans (user-based, with pearson similarity, and k=30)<br>&emsp;· KNNWithMeans (item-based, with cosine similarity, and k=20)<br>&emsp;· SVD (with n_factors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bb42f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD, KNNWithMeans, SlopeOne\n",
    "from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006412f8",
   "metadata": {},
   "source": [
    "<font size = 4>*- Perform cross validation on the following algorithms*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec9db6",
   "metadata": {},
   "source": [
    "### Slope one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8cc54748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MAE of algorithm SlopeOne on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4644  3.4297  3.4688  3.4573  3.4844  3.4609  0.0180  \n",
      "Fit time          0.06    0.06    0.05    0.05    0.06    0.05    0.00    \n",
      "Test time         0.27    0.28    0.27    0.29    0.30    0.28    0.01    \n"
     ]
    }
   ],
   "source": [
    "# SlopeOne: simple non-similarity-based algorithm with no parameters\n",
    "alg1 =SlopeOne()\n",
    "cv_results_SlopeOne = cross_validate(alg1, data, measures=['MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a36930",
   "metadata": {},
   "source": [
    "### KNN with Means, user based, pearson similarity, k=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "039c6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use user-based with Pearson correlation\n",
    "sim_options = {\n",
    "    \"name\": \"pearson\",\n",
    "    \"user_based\": True,  # Compute  similarities between users\n",
    "}\n",
    "alg2 = KNNWithMeans(k = 30, sim_options=sim_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3daf561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.3511  3.3661  3.3407  3.3367  3.3352  3.3460  0.0115  \n",
      "Fit time          0.98    0.97    0.98    0.99    0.97    0.98    0.01    \n",
      "Test time         2.56    2.45    2.48    2.39    2.37    2.45    0.07    \n"
     ]
    }
   ],
   "source": [
    "# Run 5-fold cross-validation and print results\n",
    "cv_results_userknn = cross_validate(alg2, data, measures=['MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c8395d",
   "metadata": {},
   "source": [
    "### KNN with Means, item based, cosine similarity, k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "96a55e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use item-based KNN with cosine similarity\n",
    "sim_options = {\n",
    "    \"name\": \"cosine\",\n",
    "    \"user_based\": False,  # Compute  similarities between items\n",
    "}\n",
    "alg3 = KNNWithMeans(k=20, sim_options=sim_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "414a6735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4179  3.3753  3.3734  3.3794  3.3702  3.3833  0.0176  \n",
      "Fit time          0.04    0.04    0.04    0.04    0.04    0.04    0.00    \n",
      "Test time         0.43    0.42    0.46    0.41    0.41    0.43    0.02    \n"
     ]
    }
   ],
   "source": [
    "# Run 5-fold cross-validation and print results\n",
    "cv_results_itemknn = cross_validate(alg3, data, measures=['MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1da1c6",
   "metadata": {},
   "source": [
    "### SVD, n factors = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "87f13887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.3526  3.3384  3.3565  3.3011  3.3036  3.3304  0.0237  \n",
      "Fit time          0.48    0.48    0.48    0.47    0.48    0.48    0.00    \n",
      "Test time         0.03    0.07    0.03    0.03    0.03    0.04    0.02    \n"
     ]
    }
   ],
   "source": [
    "# SVD\n",
    "alg4 = SVD(n_factors=10)\n",
    "cv_results_SVD = cross_validate(alg4, data, measures=['MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de7d8f1",
   "metadata": {},
   "source": [
    "<font size = 4>*- Get all MAEs on test set for each algorithm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7e9bd83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Slope One</th>\n",
       "      <th>User Knn</th>\n",
       "      <th>Item Knn</th>\n",
       "      <th>SVD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.46</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.43</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.47</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.46</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.48</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.37</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Slope One  User Knn  Item Knn   SVD\n",
       "0       3.46      3.35      3.42  3.35\n",
       "1       3.43      3.37      3.38  3.34\n",
       "2       3.47      3.34      3.37  3.36\n",
       "3       3.46      3.34      3.38  3.30\n",
       "4       3.48      3.34      3.37  3.30"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the results across differnt cross-validation folds\n",
    "\n",
    "box_data = pd.DataFrame({'Slope One': cv_results_SlopeOne['test_mae'],\n",
    "                         'User Knn': cv_results_userknn['test_mae'],\n",
    "                         'Item Knn': cv_results_itemknn['test_mae'],\n",
    "                         'SVD': cv_results_SVD['test_mae']})\n",
    "\n",
    "box_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3e0b9",
   "metadata": {},
   "source": [
    "<font size = 4>*- Plot the boxplot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dfade13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'MAE'), Text(0.5, 1.0, 'MAEs of Four Algorithms on 5 splits')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc = {'figure.figsize':(8,4)})\n",
    "sns.boxplot(data = box_data,color=\"skyblue\",orient=\"h\").set(xlabel=\"MAE\",title= \"MAEs of Four Algorithms on 5 splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4fe5f0",
   "metadata": {},
   "source": [
    "## 2c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc250f29",
   "metadata": {},
   "source": [
    "Write a function or code segment to run an experiment to optimize the item-based version of KNNWithMeans (with Cosine as similarity) for k. First, use Surprise's (not Scikit-learn's) train_test_split function (from surprise.model_selection) with test_size=0.2 to split the data. Then perform cross-validation on the training portion multiple times for a range of values of k and record the mean MAE values. Plot the mean cross-validation MAE against the values of k (this should look similar to the graph in Part 1(d)). Finally, use the best k value to to compute the performance of KNNWithMeans on the test set predictions (using the \"accuracy\" module in Surprise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2df344c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "train,test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74290603",
   "metadata": {},
   "source": [
    "<font size = 4>*- The Trainset object is a more complex object than the Dataset object, and it cannot be used in cross validation here. Using random to manually split the dataset here*  \n",
    "    \n",
    "<font size = 3>Ref: https://datascience.stackexchange.com/questions/73583/how-to-train-test-split-and-cross-validate-in-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "da0277fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56540"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "raw_ratings = data.raw_ratings                                             \n",
    "train, test = data,data\n",
    "# shuffle ratings dataset                                            \n",
    "random.shuffle(raw_ratings) \n",
    "# find the 80% split point\n",
    "split_point = int(len(raw_ratings)//(1/0.8))\n",
    "split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "776685c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set \n",
    "trainset_raw_ratings = raw_ratings[:split_point]                             \n",
    "testset_raw_ratings = raw_ratings[split_point:]  \n",
    "\n",
    "train.raw_ratings = trainset_raw_ratings\n",
    "test.raw_rating = testset_raw_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dbcfaf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     4.1291  4.0364  4.0402  4.1078  4.1125  4.0852  0.0390  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.18    0.13    0.13    0.13    0.12    0.14    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.7012  3.7152  3.7219  3.6917  3.7129  3.7086  0.0108  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.21    0.16    0.15    0.15    0.16    0.17    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.5995  3.6172  3.5494  3.5736  3.5602  3.5800  0.0250  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.22    0.16    0.16    0.16    0.16    0.17    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4819  3.4900  3.5158  3.5306  3.5248  3.5086  0.0193  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.21    0.17    0.17    0.17    0.17    0.18    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4799  3.4878  3.4920  3.4992  3.4369  3.4792  0.0220  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.23    0.18    0.18    0.18    0.19    0.19    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4692  3.4126  3.4592  3.4284  3.4826  3.4504  0.0260  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.24    0.19    0.18    0.19    0.20    0.20    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4227  3.4504  3.4610  3.4224  3.4439  3.4401  0.0153  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.25    0.20    0.20    0.20    0.20    0.21    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4189  3.4337  3.3930  3.4553  3.4279  3.4258  0.0203  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.26    0.21    0.21    0.21    0.20    0.22    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4125  3.4773  3.3986  3.4029  3.4111  3.4205  0.0289  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.26    0.21    0.22    0.22    0.22    0.23    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4462  3.4092  3.4224  3.3658  3.4400  3.4167  0.0286  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.24    0.23    0.23    0.23    0.23    0.23    0.00    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4117  3.4248  3.3946  3.4287  3.4388  3.4197  0.0153  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.30    0.23    0.25    0.24    0.25    0.25    0.03    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.3980  3.4488  3.3896  3.4006  3.4406  3.4155  0.0242  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.29    0.24    0.24    0.25    0.25    0.25    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4457  3.4028  3.3918  3.4097  3.4265  3.4153  0.0189  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.29    0.26    0.26    0.27    0.24    0.26    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.3960  3.4037  3.4284  3.4266  3.3825  3.4074  0.0177  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.31    0.27    0.25    0.25    0.25    0.26    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4123  3.3962  3.4015  3.4241  3.4074  3.4083  0.0096  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.30    0.27    0.27    0.26    0.26    0.27    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4072  3.4188  3.3887  3.4388  3.3863  3.4079  0.0195  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.26    0.26    0.26    0.26    0.26    0.26    0.00    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4228  3.3982  3.4006  3.3929  3.4213  3.4072  0.0124  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.32    0.28    0.27    0.27    0.27    0.28    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4203  3.3758  3.3786  3.4300  3.4325  3.4074  0.0250  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.28    0.28    0.28    0.28    0.28    0.28    0.00    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4177  3.3856  3.4468  3.4566  3.3687  3.4151  0.0339  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.33    0.29    0.30    0.30    0.29    0.30    0.01    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.3888  3.4423  3.3847  3.3785  3.4284  3.4045  0.0257  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.30    0.29    0.29    0.29    0.29    0.29    0.00    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4129  3.4437  3.4138  3.3901  3.3811  3.4083  0.0218  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.30    0.30    0.29    0.29    0.29    0.29    0.00    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4115  3.3838  3.4224  3.3993  3.4196  3.4073  0.0142  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.30    0.30    0.30    0.30    0.30    0.30    0.00    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4391  3.3971  3.3989  3.4144  3.3925  3.4084  0.0170  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.34    0.30    0.30    0.30    0.31    0.31    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.3872  3.4212  3.4156  3.4465  3.3807  3.4103  0.0240  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.35    0.32    0.31    0.31    0.31    0.32    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4277  3.3827  3.4036  3.3975  3.4500  3.4123  0.0238  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.36    0.32    0.32    0.32    0.31    0.32    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.3606  3.4369  3.4300  3.4125  3.4015  3.4083  0.0269  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.38    0.39    0.36    0.43    0.34    0.38    0.03    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.3917  3.4579  3.4324  3.3981  3.3892  3.4138  0.0269  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.39    0.32    0.37    0.32    0.32    0.35    0.03    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4428  3.3923  3.4615  3.3777  3.4081  3.4165  0.0312  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.32    0.32    0.33    0.33    0.32    0.33    0.00    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4316  3.4033  3.4151  3.4159  3.4187  3.4169  0.0090  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.37    0.33    0.33    0.33    0.33    0.34    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4289  3.4352  3.4344  3.3983  3.3937  3.4181  0.0182  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.33    0.33    0.33    0.33    0.34    0.33    0.00    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.3957  3.4223  3.4171  3.4463  3.4059  3.4175  0.0171  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.39    0.34    0.34    0.34    0.34    0.35    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4305  3.4398  3.4216  3.4258  3.3914  3.4218  0.0164  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.34    0.34    0.34    0.34    0.34    0.34    0.00    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4095  3.4471  3.4138  3.4581  3.4050  3.4267  0.0216  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.34    0.34    0.37    0.36    0.36    0.35    0.01    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4348  3.4344  3.4195  3.4213  3.4106  3.4241  0.0093  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.37    0.35    0.34    0.34    0.35    0.35    0.01    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4299  3.4222  3.4527  3.4107  3.4336  3.4298  0.0138  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.39    0.35    0.35    0.35    0.35    0.35    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.3944  3.4515  3.4419  3.4474  3.4065  3.4283  0.0233  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.39    0.35    0.35    0.35    0.35    0.36    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4660  3.4022  3.4512  3.4091  3.4265  3.4310  0.0243  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.39    0.35    0.35    0.35    0.35    0.36    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.3987  3.4620  3.4202  3.4485  3.4288  3.4316  0.0220  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.39    0.35    0.36    0.36    0.35    0.36    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4355  3.4389  3.4331  3.4611  3.4094  3.4356  0.0165  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.40    0.35    0.36    0.36    0.36    0.37    0.02    \n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating MAE of algorithm KNNWithMeans on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "MAE (testset)     3.4581  3.4430  3.4232  3.4066  3.4460  3.4354  0.0182  \n",
      "Fit time          0.03    0.03    0.03    0.03    0.03    0.03    0.00    \n",
      "Test time         0.36    0.36    0.36    0.36    0.36    0.36    0.00    \n"
     ]
    }
   ],
   "source": [
    "# To use item-based KNN with cosine similarity\n",
    "sim_options = {\n",
    "    \"name\": \"cosine\",\n",
    "    \"user_based\": False,  \n",
    "}\n",
    "MAE = np.array([]) \n",
    "for k in range(1,41):\n",
    "    algo = KNNWithMeans(k=k, sim_options=sim_options)\n",
    "    cv_results_itemknn = cross_validate(algo, train, measures=['MAE'], cv=5, verbose=True)\n",
    "    mean = np.mean(cv_results_itemknn['test_mae'])\n",
    "    MAE = np.append(MAE,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "927918b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(range(1,41))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901f73e",
   "metadata": {},
   "source": [
    "<font size = 4>*- Show the MAE plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "75d73c53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'k value'),\n",
       " Text(0, 0.5, 'MAE'),\n",
       " Text(0.5, 1.0, 'MAE under different k value')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x = k,y = MAE).set(xlabel='k value', ylabel = 'MAE', title = 'MAE under different k value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3874bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best k as in min MAE \n",
    "k_best = np.where(MAE == MAE.min())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3ad19d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 4.2957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.295700525362834"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = KNNWithMeans(k_best, sim_options=sim_options)\n",
    "\n",
    "# retrain on the whole train set\n",
    "trainset = train.build_full_trainset()\n",
    "alg.fit(trainset) # suppose alg is the model created with the optimal k value\n",
    "\n",
    "# now test on the testset                                                  \n",
    "testset = test.construct_testset(testset_raw_ratings)                         \n",
    "predictions = alg.test(testset)                                           \n",
    "                                         \n",
    "accuracy.rmse(predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593bdcd",
   "metadata": {},
   "source": [
    "<font size = 4>*The RMSE on test set shows above*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0737e0f",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
